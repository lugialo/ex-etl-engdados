{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Boas-vindas \u00e0 Documenta\u00e7\u00e3o do Pipeline ETL de Cl\u00ednicas Odontol\u00f3gicas","text":"<p>Este documento serve como guia completo para o nosso projeto de Engenharia de Dados, focado na constru\u00e7\u00e3o de um pipeline ETL robusto para dados de cl\u00ednicas odontol\u00f3gicas. Aqui voc\u00ea encontrar\u00e1 desde a vis\u00e3o geral da arquitetura at\u00e9 os detalhes t\u00e9cnicos de cada etapa de transforma\u00e7\u00e3o de dados.</p> <p>O objetivo principal deste projeto \u00e9 simular um ambiente real de processamento de dados, aplicando conceitos de engenharia de dados para transformar informa\u00e7\u00f5es brutas de diversas fontes (inicialmente arquivos CSV) em um formato estruturado e otimizado para an\u00e1lise. Utilizamos a metodologia de arquitetura medalh\u00e3o para garantir a qualidade e a rastreabilidade dos dados em diferentes est\u00e1gios:</p> <ul> <li>Camada Landing (Raw): Dados brutos, como foram recebidos das fontes.</li> <li>Camada Bronze: Dados brutos ingeridos, com m\u00ednimas valida\u00e7\u00f5es de formato.</li> <li>Camada Silver: Dados limpos, transformados e padronizados, prontos para serem usados em an\u00e1lises mais detalhadas.</li> <li>Camada Gold: Dados agregados e modelados em um formato dimensional (Data Warehouse), otimizados para consumo por ferramentas de BI e an\u00e1lise de neg\u00f3cios.</li> </ul> <p>Esta documenta\u00e7\u00e3o ir\u00e1 gui\u00e1-lo atrav\u00e9s da arquitetura do pipeline, das ferramentas utilizadas, das transforma\u00e7\u00f5es aplicadas em cada etapa e de como executar o projeto em seu ambiente local.</p>"},{"location":"como_executar/","title":"Como Executar o Projeto","text":"<p>Este guia completo mostra como executar o projeto ETL de cl\u00ednicas odontol\u00f3gicas do in\u00edcio ao fim.</p>"},{"location":"como_executar/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de ter instalado:</p> <ul> <li>Python 3.9+</li> <li>Docker e Docker Compose</li> <li>Git</li> <li>Azure CLI (para deploy na nuvem)</li> <li>Terraform (para infraestrutura)</li> </ul>"},{"location":"como_executar/#configuracao-inicial","title":"Configura\u00e7\u00e3o Inicial","text":""},{"location":"como_executar/#1-clone-e-configure-o-projeto","title":"1. Clone e Configure o Projeto","text":"<pre><code># Clonar o reposit\u00f3rio\ngit clone https://github.com/lugialo/ex-etl-engdados.git\ncd ex-etl-engdados\n\n# Criar ambiente virtual\npython -m venv venv\nsource venv/bin/activate  # Linux/macOS\n# ou .\\venv\\Scripts\\activate  # Windows\n\n# Instalar depend\u00eancias\npip install -r requirements.txt\n</code></pre>"},{"location":"como_executar/#2-configurar-variaveis-de-ambiente","title":"2. Configurar Vari\u00e1veis de Ambiente","text":"<pre><code># Copiar arquivo de exemplo\ncp .env.example .env\n\n# Editar configura\u00e7\u00f5es (use seu editor preferido)\nnano .env\n</code></pre> <p>Configura\u00e7\u00e3o m\u00ednima para execu\u00e7\u00e3o local: <pre><code># Configura\u00e7\u00f5es do PostgreSQL (usando Docker)\nDB_HOST=localhost\nDB_PORT=5432\nDB_NAME=clinica_odonto\nDB_USER=root\nDB_PASSWORD=root\n\n# Azure Storage (opcional - deixe vazio para usar apenas localmente)\nAZURE_STORAGE_CONNECTION_STRING=\nAZURE_CONTAINER_NAME=landingzone\n</code></pre></p>"},{"location":"como_executar/#3-subir-o-banco-de-dados","title":"3. Subir o Banco de Dados","text":"<pre><code># Subir PostgreSQL via Docker\ncd docker\ndocker-compose up -d\ncd ..\n\n# Verificar se est\u00e1 rodando\ndocker-compose -f docker/docker-compose.yml ps\n</code></pre>"},{"location":"como_executar/#4-criar-estrutura-do-banco","title":"4. Criar Estrutura do Banco","text":"<pre><code># Executar scripts de cria\u00e7\u00e3o\npsql -h localhost -U root -d clinica_odonto -f scripts/modelo_fisico.sql\npsql -h localhost -U root -d clinica_odonto -f scripts/modelo_dimensional.sql\n</code></pre>"},{"location":"como_executar/#execucao-do-pipeline","title":"Execu\u00e7\u00e3o do Pipeline","text":""},{"location":"como_executar/#opcao-1-execucao-completa-automatizada","title":"Op\u00e7\u00e3o 1: Execu\u00e7\u00e3o Completa Automatizada","text":"<pre><code># Gerar dados e executar todo o pipeline\npython scripts/gerador_dados.py\n\n# Abrir Jupyter Lab\njupyter lab\n\n# Executar notebooks na ordem:\n# 1. notebook_landing_bronze.ipynb\n# 2. notebook_bronze_silver.ipynb  \n# 3. notebook_silver_gold.ipynb\n</code></pre>"},{"location":"como_executar/#opcao-2-execucao-passo-a-passo","title":"Op\u00e7\u00e3o 2: Execu\u00e7\u00e3o Passo a Passo","text":""},{"location":"como_executar/#passo-1-gerar-dados-brutos","title":"Passo 1: Gerar Dados Brutos","text":"<pre><code>python scripts/gerador_dados.py\n</code></pre> <p>O que acontece: - \u2705 Conecta ao banco PostgreSQL - \u2705 Cria e popula tabelas com dados sint\u00e9ticos - \u2705 Exporta dados para CSV na pasta <code>data/raw/</code> - \u2705 Upload para Azure Storage (se configurado) - \u2705 Logs detalhados do processo</p>"},{"location":"como_executar/#passo-2-landing-bronze","title":"Passo 2: Landing \u2192 Bronze","text":"<pre><code>jupyter lab notebooks/notebook_landing_bronze.ipynb\n</code></pre> <p>O que acontece: - \ud83d\udcc2 L\u00ea CSVs da pasta <code>data/raw/</code> - \ud83d\udd27 Aplica tipagem b\u00e1sica aos dados - \ud83d\udcbe Salva dados estruturados em <code>data/bronze/</code></p>"},{"location":"como_executar/#passo-3-bronze-silver","title":"Passo 3: Bronze \u2192 Silver","text":"<pre><code>jupyter lab notebooks/notebook_bronze_silver.ipynb\n</code></pre> <p>O que acontece: - \ud83e\uddf9 Limpa e valida dados - \ud83d\udd04 Padroniza formatos - \u2705 Remove duplicatas e inconsist\u00eancias - \ud83d\udcbe Salva dados limpos em <code>data/silver/</code></p>"},{"location":"como_executar/#passo-4-silver-gold","title":"Passo 4: Silver \u2192 Gold","text":"<pre><code>jupyter lab notebooks/notebook_silver_gold.ipynb\n</code></pre> <p>O que acontece: - \ud83d\udcca Agrega dados por dimens\u00f5es de neg\u00f3cio - \ud83c\udfd7\ufe0f Cria modelo dimensional (fatos e dimens\u00f5es) - \ud83d\udcbe Carrega dados no Data Warehouse (PostgreSQL) - \ud83d\udcbe Salva agrega\u00e7\u00f5es em <code>data/gold/</code></p>"},{"location":"como_executar/#validacao-dos-resultados","title":"Valida\u00e7\u00e3o dos Resultados","text":""},{"location":"como_executar/#verificar-dados-gerados","title":"Verificar Dados Gerados","text":"<pre><code># Ver arquivos CSV criados\nls -la data/raw/\n\n# Ver estrutura das camadas\ntree data/\n</code></pre>"},{"location":"como_executar/#consultar-banco-de-dados","title":"Consultar Banco de Dados","text":"<pre><code># Conectar ao banco\npsql -h localhost -U root -d clinica_odonto\n\n# Exemplos de queries\nSELECT COUNT(*) FROM dim_paciente;\nSELECT COUNT(*) FROM fato_consulta;\nSELECT * FROM dim_tempo LIMIT 5;\n</code></pre>"},{"location":"como_executar/#verificar-logs","title":"Verificar Logs","text":"<pre><code># Ver logs do Docker\ndocker-compose -f docker/docker-compose.yml logs -f\n\n# Ver logs do gerador de dados\n# (logs s\u00e3o exibidos no terminal durante execu\u00e7\u00e3o)\n</code></pre>"},{"location":"como_executar/#execucao-com-azure-opcional","title":"Execu\u00e7\u00e3o com Azure (Opcional)","text":""},{"location":"como_executar/#1-deploy-da-infraestrutura","title":"1. Deploy da Infraestrutura","text":"<pre><code># Fazer login na Azure\naz login\n\n# Configurar Terraform\ncd terraform\nterraform init\nterraform plan\nterraform apply\n\n# Obter connection string\nterraform output -raw storage_connection_string\n</code></pre>"},{"location":"como_executar/#2-atualizar-configuracao","title":"2. Atualizar Configura\u00e7\u00e3o","text":"<pre><code># Atualizar .env com credenciais da Azure\nAZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=...\"\nAZURE_CONTAINER_NAME=landingzone\n</code></pre>"},{"location":"como_executar/#3-executar-com-azure-storage","title":"3. Executar com Azure Storage","text":"<pre><code># Gerar dados (agora com upload para Azure)\npython scripts/gerador_dados.py\n\n# Os CSVs ser\u00e3o salvos localmente E na Azure\n</code></pre>"},{"location":"como_executar/#monitoramento-e-debugging","title":"Monitoramento e Debugging","text":""},{"location":"como_executar/#verificar-status-dos-servicos","title":"Verificar Status dos Servi\u00e7os","text":"<pre><code># Docker\ndocker-compose -f docker/docker-compose.yml ps\n\n# Conectividade com banco\npython scripts/teste_db.py\n\n# Azure Storage (se configurado)\naz storage container list --connection-string \"sua_connection_string\"\n</code></pre>"},{"location":"como_executar/#logs-detalhados","title":"Logs Detalhados","text":"<pre><code># Logs do PostgreSQL\ndocker-compose -f docker/docker-compose.yml logs db\n\n# Logs espec\u00edficos do gerador\npython scripts/gerador_dados.py 2&gt;&amp;1 | tee gerador.log\n</code></pre>"},{"location":"como_executar/#resolver-problemas-comuns","title":"Resolver Problemas Comuns","text":""},{"location":"como_executar/#erro-de-conexao-com-banco","title":"Erro de Conex\u00e3o com Banco","text":"<pre><code># Verificar se container est\u00e1 rodando\ndocker ps | grep postgres\n\n# Reiniciar se necess\u00e1rio\ndocker-compose -f docker/docker-compose.yml restart\n</code></pre>"},{"location":"como_executar/#erro-de-permissao-no-azure","title":"Erro de Permiss\u00e3o no Azure","text":"<pre><code># Verificar login\naz account show\n\n# Verificar acesso ao storage\naz storage account show --name seu_storage_account\n</code></pre>"},{"location":"como_executar/#erro-de-dependencias-python","title":"Erro de Depend\u00eancias Python","text":"<pre><code># Reinstalar depend\u00eancias\npip install -r requirements.txt --force-reinstall\n\n# Verificar vers\u00f5es\npip list | grep -E \"(pandas|sqlalchemy|azure)\"\n</code></pre>"},{"location":"como_executar/#performance-e-otimizacao","title":"Performance e Otimiza\u00e7\u00e3o","text":""},{"location":"como_executar/#configuracoes-para-datasets-maiores","title":"Configura\u00e7\u00f5es para Datasets Maiores","text":"<p>No arquivo <code>scripts/gerador_dados.py</code>, ajuste:</p> <pre><code># Aumentar n\u00famero de registros\nNUM_ENDERECOS = 50000\nNUM_PACIENTES = 100000\nNUM_CONSULTAS = 500000\n</code></pre>"},{"location":"como_executar/#paralelizacao-com-databricks","title":"Paraleliza\u00e7\u00e3o com Databricks","text":"<ol> <li>Deploy infraestrutura Terraform</li> <li>Configure cluster Databricks</li> <li>Upload notebooks para Databricks</li> <li>Execute com maior poder computacional</li> </ol>"},{"location":"como_executar/#documentacao-e-visualizacao","title":"Documenta\u00e7\u00e3o e Visualiza\u00e7\u00e3o","text":""},{"location":"como_executar/#gerar-documentacao","title":"Gerar Documenta\u00e7\u00e3o","text":"<pre><code># Servidor local da documenta\u00e7\u00e3o\nmkdocs serve\n\n# Acesse: http://127.0.0.1:8000\n</code></pre>"},{"location":"como_executar/#build-para-producao","title":"Build para Produ\u00e7\u00e3o","text":"<pre><code># Build est\u00e1tico\nmkdocs build\n\n# Deploy para GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"como_executar/#limpeza-do-ambiente","title":"Limpeza do Ambiente","text":""},{"location":"como_executar/#parar-servicos","title":"Parar Servi\u00e7os","text":"<pre><code># Parar Docker\ndocker-compose -f docker/docker-compose.yml down\n\n# Desativar ambiente virtual\ndeactivate\n</code></pre>"},{"location":"como_executar/#limpeza-completa","title":"Limpeza Completa","text":"<pre><code># Remover dados locais\nrm -rf data/bronze/ data/silver/ data/gold/\n\n# Remover volumes Docker\ndocker-compose -f docker/docker-compose.yml down -v\n\n# Destruir infraestrutura Azure\ncd terraform\nterraform destroy\n</code></pre>"},{"location":"sobre/","title":"Sobre o Projeto","text":"<p>Este projeto de Engenharia de Dados foi desenvolvido como parte do trabalho final da disciplina de Engenharia de Dados do curso de Engenharia de Software da UNISATC.</p> <p>Objetivos de Aprendizado:</p> <p>O principal objetivo deste trabalho foi aplicar na pr\u00e1tica os conceitos e metodologias da Engenharia de Dados, incluindo:</p> <ul> <li>Modelagem de Dados: Compreens\u00e3o e aplica\u00e7\u00e3o de modelos de dados relacionais e dimensionais.</li> <li>Pipeline ETL: Constru\u00e7\u00e3o de um fluxo completo de Extra\u00e7\u00e3o, Transforma\u00e7\u00e3o e Carga de dados.</li> <li>Arquitetura de Dados: Implementa\u00e7\u00e3o da arquitetura medalh\u00e3o (Landing, Bronze, Silver, Gold) em um Data Lake.</li> <li>Ferramentas de Big Data: Utiliza\u00e7\u00e3o de ferramentas e linguagens como Python e SQL para manipula\u00e7\u00e3o e processamento de grandes volumes de dados.</li> <li>Documenta\u00e7\u00e3o T\u00e9cnica: Cria\u00e7\u00e3o de uma documenta\u00e7\u00e3o clara e abrangente para o projeto, facilitando o entendimento e a manuten\u00e7\u00e3o.</li> </ul> <p>Equipe de Desenvolvimento:</p> <p>Este projeto foi desenvolvido por:</p> <p>Anna Clara Cau\u00e3 Loch Gabriel Antonin Gabrielle Coelho Vin\u00edcius Teixeira Colombo</p> <p>Agradecimentos:</p> <p>Gostar\u00edamos de expressar nossa gratid\u00e3o ao(\u00e0) professor(a) respons\u00e1vel pela disciplina de Engenharia de Dados na UNISATC, Jorge Luiz Da Silva, por sua orienta\u00e7\u00e3o e suporte ao longo do desenvolvimento deste projeto.</p>"},{"location":"camadas/bronze/","title":"Camada Bronze","text":"<p>A camada Bronze do nosso Data Lake representa o primeiro est\u00e1gio de ingest\u00e3o e valida\u00e7\u00e3o inicial dos dados brutos que v\u00eam da camada Landing. Embora os dados aqui ainda sejam \"crus\" em termos de transforma\u00e7\u00f5es de neg\u00f3cio, eles j\u00e1 passaram por um processo b\u00e1sico de garantia de que s\u00e3o leg\u00edveis e v\u00e1lidos em seu formato.</p> <p>Prop\u00f3sito:</p> <ul> <li>Ingest\u00e3o Validada: Ler os arquivos da camada Landing e salv\u00e1-los novamente, garantindo que o processo de leitura e escrita n\u00e3o introduziu erros e que os dados est\u00e3o em um formato acess\u00edvel para processamento posterior.</li> <li>Armazenamento Consistente: Padronizar o formato de armazenamento (se houver necessidade de convers\u00e3o, por exemplo, de CSV para Parquet, embora neste projeto mantenhamos CSV para simplicidade).</li> <li>Ponto de Partida: Servir como o ponto de partida para todas as transforma\u00e7\u00f5es de limpeza e enriquecimento que ocorrer\u00e3o na camada Silver.</li> </ul> <p>Caracter\u00edsticas:</p> <ul> <li>Conte\u00fado Quase Bruto: Os dados s\u00e3o quase id\u00eanticos aos da camada Landing, mas com a garantia de que foram lidos e escritos com sucesso. Pequenas corre\u00e7\u00f5es de formata\u00e7\u00e3o de arquivo podem ocorrer, mas nenhuma regra de neg\u00f3cio \u00e9 aplicada nesta fase.</li> <li>Rastreabilidade: Permite a rastreabilidade entre os dados brutos e os dados prontos para transforma\u00e7\u00e3o.</li> <li>Estrutura de Armazenamento:<ul> <li>Os dados s\u00e3o persistidos na pasta <code>data/bronze/</code>.</li> <li>Cada tipo de entidade (paciente, consulta, etc.) ter\u00e1 seu arquivo CSV correspondente aqui, espelhando a estrutura da camada Landing.</li> </ul> </li> </ul> <p>Processo Detalhado (via <code>notebook_landing_bronze.ipynb</code>):</p> <p>O notebook <code>notebook_landing_bronze.ipynb</code> \u00e9 respons\u00e1vel por esta etapa. Ele realiza as seguintes opera\u00e7\u00f5es:</p> <ol> <li>Leitura da Camada Landing: L\u00ea os arquivos CSV presentes na pasta <code>data/raw/</code>.</li> <li>Verifica\u00e7\u00e3o B\u00e1sica: Realiza uma verifica\u00e7\u00e3o b\u00e1sica da estrutura dos arquivos, garantindo que eles podem ser carregados corretamente em DataFrames do Pandas.</li> <li>Persist\u00eancia na Camada Bronze: Salva os DataFrames resultantes como novos arquivos CSV na pasta <code>data/bronze/</code>. Isso garante que qualquer erro de formato que impe\u00e7a a leitura do arquivo raw seja identificado, e que a pr\u00f3xima etapa n\u00e3o dependa da leitura direta dos arquivos originais.</li> </ol>"},{"location":"camadas/gold/","title":"Camada Gold (Data Warehouse)","text":"<p>A camada Gold \u00e9 a camada mais refinada e estrat\u00e9gica do nosso Data Lake. Ela cont\u00e9m dados que foram completamente processados, limpos, transformados, agregados e modelados em um formato otimizado para consumo por usu\u00e1rios de neg\u00f3cios, analistas de dados e ferramentas de Business Intelligence (BI). \u00c9 aqui que os dados se transformam em informa\u00e7\u00f5es acion\u00e1veis.</p> <p>Prop\u00f3sito:</p> <ul> <li>Consumo Anal\u00edtico: Fornecer um modelo de dados perform\u00e1tico e intuitivo para consultas anal\u00edticas complexas e gera\u00e7\u00e3o de relat\u00f3rios e dashboards.</li> <li>Decis\u00e3o de Neg\u00f3cios: Ser a fonte de verdade para todas as an\u00e1lises de neg\u00f3cio, KPIs (Key Performance Indicators) e m\u00e9tricas.</li> <li>Simplicidade para o Usu\u00e1rio Final: Apresentar os dados de forma desnormalizada e f\u00e1cil de entender, minimizando a necessidade de conhecimento t\u00e9cnico em SQL ou modelagem de dados para os usu\u00e1rios de BI.</li> </ul> <p>Caracter\u00edsticas:</p> <ul> <li>Modelagem Dimensional: Os dados s\u00e3o organizados em um esquema estrela (Star Schema), composto por tabelas de fato e tabelas de dimens\u00e3o.<ul> <li>Tabelas de Fato: Cont\u00eam as m\u00e9tricas e os fatos de neg\u00f3cio (por exemplo, valores de pagamento, dura\u00e7\u00e3o da consulta, quantidade de procedimentos). Elas possuem chaves estrangeiras para as tabelas de dimens\u00e3o.</li> <li>Tabelas de Dimens\u00e3o: Cont\u00eam os atributos descritivos do neg\u00f3cio (por exemplo, detalhes do paciente, informa\u00e7\u00f5es do odontologista, datas).</li> </ul> </li> <li>Alta Qualidade e Consist\u00eancia: Herda a qualidade e padroniza\u00e7\u00e3o da camada Silver, com dados prontos para an\u00e1lise imediata.</li> <li>Otimiza\u00e7\u00e3o para Leitura: Estruturada para otimizar o desempenho de consultas de leitura e agrega\u00e7\u00e3o.</li> <li>Armazenamento: Os dados da camada Gold s\u00e3o carregados em um Banco de Dados Relacional (PostgreSQL), garantindo a integridade e a capacidade de consulta via SQL. Embora dados intermedi\u00e1rios possam existir em <code>data/gold/</code>, o principal destino de consumo \u00e9 o banco de dados.</li> </ul> <p>Exemplo de Modelo Dimensional (Schema Estrela - simplificado):</p> <p>Em nosso projeto, esperamos ter tabelas de fato e dimens\u00e3o semelhantes a:</p> <ul> <li> <p>Fato:</p> <ul> <li><code>fato_consultas_e_pagamentos</code>: Cont\u00e9m m\u00e9tricas sobre consultas e pagamentos (valor total, dura\u00e7\u00e3o, quantidade de procedimentos) e chaves para dimens\u00f5es.</li> </ul> </li> <li> <p>Dimens\u00f5es:</p> <ul> <li><code>dim_tempo</code>: Detalhes sobre datas (ano, m\u00eas, dia, dia da semana, feriado, etc.).</li> <li><code>dim_paciente</code>: Informa\u00e7\u00f5es detalhadas sobre os pacientes (nome, data de nascimento, g\u00eanero, telefone, endere\u00e7o).</li> <li><code>dim_odontologista</code>: Dados sobre os profissionais (nome, especialidade, CRO).</li> <li><code>dim_procedimento</code>: Detalhes dos procedimentos (nome do procedimento, descri\u00e7\u00e3o, custo padr\u00e3o).</li> <li><code>dim_tipo_pagamento</code>: Descri\u00e7\u00e3o dos tipos de pagamento.</li> </ul> </li> </ul> <p>Processo Detalhado (via <code>notebook_silver_gold.ipynb</code>):</p> <p>O notebook <code>notebook_silver_gold.ipynb</code> executa as seguintes a\u00e7\u00f5es para construir e popular a camada Gold:</p> <ol> <li>Leitura da Camada Silver: Carrega os DataFrames limpos e transformados da pasta <code>data/silver/</code>.</li> <li>Constru\u00e7\u00e3o das Dimens\u00f5es:<ul> <li>Os dados de cada entidade s\u00e3o extra\u00eddos e transformados para se adequarem \u00e0s tabelas de dimens\u00e3o.</li> <li>Garantia de unicidade para as dimens\u00f5es.</li> <li>Exemplo: A <code>dim_tempo</code> \u00e9 geralmente gerada ou preenchida com anteced\u00eancia, ou criada a partir das datas existentes nas transa\u00e7\u00f5es.</li> </ul> </li> <li>Constru\u00e7\u00e3o da Tabela de Fato:<ul> <li>As tabelas de dimens\u00e3o s\u00e3o unidas aos dados transacionais da camada Silver.</li> <li>C\u00e1lculo de m\u00e9tricas (valores totais, contagens, etc.).</li> <li>As chaves prim\u00e1rias das dimens\u00f5es s\u00e3o obtidas e inseridas como chaves estrangeiras na tabela de fato.</li> </ul> </li> <li>Carregamento no Banco de Dados:<ul> <li>Utiliza a biblioteca <code>SQLAlchemy</code> ou <code>psycopg2</code> (para PostgreSQL) para conectar ao banco de dados.</li> <li>Os scripts SQL em <code>scripts/modelo_fisico.sql</code> e <code>scripts/modelo_dimensional.sql</code> s\u00e3o usados para criar as tabelas no banco de dados.</li> <li>Os DataFrames resultantes das tabelas de fato e dimens\u00e3o s\u00e3o carregados (inseridos) nas tabelas correspondentes do banco de dados PostgreSQL.</li> </ul> </li> </ol> <p>Esta camada \u00e9 o produto final do pipeline ETL, fornecendo uma base s\u00f3lida para qualquer necessidade de an\u00e1lise de dados.</p>"},{"location":"camadas/landing/","title":"Camada Landing (Raw)","text":"<p>A camada Landing, tamb\u00e9m conhecida como Raw, \u00e9 a primeira etapa do nosso Data Lake. Seu principal objetivo \u00e9 receber os dados diretamente das fontes de origem, sem qualquer tipo de transforma\u00e7\u00e3o, limpeza ou valida\u00e7\u00e3o de conte\u00fado.</p> <p>Prop\u00f3sito:</p> <ul> <li>Ingest\u00e3o Bruta: Servir como o ponto de entrada inicial para todos os dados provenientes das fontes.</li> <li>Preserva\u00e7\u00e3o: Manter os dados em seu formato original e intacto, exatamente como foram gerados pela fonte. Isso \u00e9 crucial para auditoria, rastreabilidade e para garantir que, se houver um erro em etapas posteriores, podemos sempre retornar \u00e0 fonte original.</li> <li>Imutabilidade: Os dados nesta camada s\u00e3o considerados imut\u00e1veis. Uma vez que um arquivo \u00e9 gravado na Landing Zone, ele n\u00e3o deve ser alterado.</li> </ul> <p>Caracter\u00edsticas:</p> <ul> <li>Formato: Os dados s\u00e3o armazenados em arquivos CSV, replicando a estrutura da fonte de origem.</li> <li>Conte\u00fado: Cont\u00e9m todos os dados, incluindo poss\u00edveis inconsist\u00eancias, erros ou duplica\u00e7\u00f5es que existiam na fonte original.</li> <li>Estrutura de Armazenamento:<ul> <li>Os dados s\u00e3o organizados em subpastas, como <code>data/raw/</code>.</li> <li>Cada tipo de entidade (por exemplo, <code>paciente.csv</code>, <code>consulta.csv</code>, <code>odontologista.csv</code>, <code>procedimento.csv</code>, <code>pagamento.csv</code>, <code>tipo_pagamento.csv</code>, <code>endereco.csv</code>, <code>agendamento.csv</code>, <code>consulta_procedimento.csv</code>, <code>log_pagamento.csv</code>) tem seu pr\u00f3prio arquivo CSV correspondente na pasta <code>data/raw/</code>.</li> </ul> </li> </ul> <p>Nesta fase, o foco \u00e9 apenas na coleta eficiente e segura dos dados, deixando as etapas de refinamento para as camadas subsequentes do Data Lake.</p>"},{"location":"camadas/silver/","title":"Camada Silver","text":"<p>A camada Silver (tamb\u00e9m conhecida como camada Refinada ou Consolidada) \u00e9 onde os dados brutos da camada Bronze s\u00e3o transformados em um formato limpo, padronizado e validado, pronto para ser consumido em an\u00e1lises mais detalhadas ou para a constru\u00e7\u00e3o do Data Warehouse na camada Gold. Esta camada \u00e9 o cora\u00e7\u00e3o do nosso processo de ETL, pois \u00e9 aqui que as principais regras de neg\u00f3cio para tratamento de dados s\u00e3o aplicadas.</p> <p>Prop\u00f3sito:</p> <ul> <li>Limpeza e Padroniza\u00e7\u00e3o: Remover inconsist\u00eancias, tratar valores nulos, padronizar formatos (datas, textos, n\u00fameros) e corrigir erros de digita\u00e7\u00e3o ou formata\u00e7\u00e3o.</li> <li>Enriquecimento: Adicionar novas colunas ou derivar informa\u00e7\u00f5es a partir dos dados existentes, aplicando regras de neg\u00f3cio espec\u00edficas.</li> <li>Consist\u00eancia: Garantir que os dados estejam em um formato consistente e confi\u00e1vel para as etapas subsequentes do pipeline.</li> <li>Consolida\u00e7\u00e3o: Integrar dados de diferentes fontes (se aplic\u00e1vel), criando um conjunto de dados unificado e coerente.</li> </ul> <p>Caracter\u00edsticas:</p> <ul> <li>Qualidade Assegurada: Os dados nesta camada possuem alta qualidade, sendo confi\u00e1veis para uso anal\u00edtico.</li> <li>Estrutura Definida: Embora ainda possam estar em formato de arquivo (CSV), a estrutura de cada entidade j\u00e1 est\u00e1 bem definida e padronizada.</li> <li>Rastreabilidade: \u00c9 poss\u00edvel rastrear os dados at\u00e9 a camada Bronze e, consequentemente, at\u00e9 a Landing.</li> </ul> <p>Processo Detalhado (via <code>notebook_bronze_silver.ipynb</code>):</p> <p>O notebook <code>notebook_bronze_silver.ipynb</code> \u00e9 o respons\u00e1vel por orquestrar as transforma\u00e7\u00f5es necess\u00e1rias para mover os dados da camada Bronze para a Silver. As opera\u00e7\u00f5es realizadas incluem, mas n\u00e3o se limitam a:</p> <ol> <li>Carregamento: Todos os arquivos CSV da pasta <code>data/bronze/</code> s\u00e3o carregados em DataFrames do Pandas.</li> <li>Tratamento de Valores Nulos:<ul> <li>Identifica\u00e7\u00e3o e substitui\u00e7\u00e3o de valores nulos em colunas como <code>telefone</code>, <code>email</code> ou <code>data_nascimento</code> (por exemplo, preenchendo com \"N\u00e3o Informado\" ou um valor padr\u00e3o, ou removendo registros se a informa\u00e7\u00e3o for cr\u00edtica).</li> <li>Exemplo: Colunas de endere\u00e7o podem ter nulos preenchidos com \"N/A\" ou inferidos.</li> </ul> </li> <li>Padroniza\u00e7\u00e3o de Formatos:<ul> <li>Datas: Convers\u00e3o de todas as colunas de data (e.g., <code>data_consulta</code>, <code>data_agendamento</code>, <code>data_nascimento</code>) para um formato uniforme (Ex: <code>YYYY-MM-DD</code>).</li> <li>Strings: Normaliza\u00e7\u00e3o de campos de texto (e.g., <code>nome_paciente</code>, <code>nome_odontologista</code>) para um padr\u00e3o consistente (Ex: t\u00edtulo, mai\u00fasculas, min\u00fasculas).</li> <li>Valores Num\u00e9ricos: Garantir que colunas num\u00e9ricas estejam no tipo de dado correto e sem valores inv\u00e1lidos.</li> </ul> </li> <li>Remo\u00e7\u00e3o de Duplicatas: Elimina\u00e7\u00e3o de registros duplicados com base em chaves de identifica\u00e7\u00e3o (e.g., <code>id_paciente</code>, <code>id_odontologista</code>) para garantir a unicidade.</li> <li>Valida\u00e7\u00e3o e Consist\u00eancia de Dados:<ul> <li>Verifica\u00e7\u00e3o de integridade referencial b\u00e1sica (e.g., se um <code>id_paciente</code> em <code>agendamento</code> realmente existe na tabela de pacientes).</li> <li>Aplica\u00e7\u00e3o de regras de neg\u00f3cio (e.g., verificar se a <code>data_agendamento</code> \u00e9 anterior \u00e0 <code>data_consulta</code>).</li> </ul> </li> <li>Enriquecimento de Dados:<ul> <li>Cria\u00e7\u00e3o de novas colunas derivadas, como <code>idade</code> a partir de <code>data_nascimento</code> ou <code>dia_semana</code> a partir de <code>data_consulta</code>.</li> <li>Combina\u00e7\u00e3o de tabelas (joins) para adicionar informa\u00e7\u00f5es relevantes. Por exemplo, unindo <code>pagamento</code> com <code>tipo_pagamento</code> para adicionar a descri\u00e7\u00e3o do tipo de pagamento.</li> </ul> </li> <li>Persist\u00eancia: Os DataFrames transformados e limpos s\u00e3o salvos como novos arquivos CSV na pasta <code>data/silver/</code>, prontos para a pr\u00f3xima etapa.</li> </ol>"},{"location":"configuracao/ambiente/","title":"Configura\u00e7\u00e3o de Ambiente","text":"<p>Este guia detalha como configurar o ambiente de desenvolvimento para o projeto ETL de cl\u00ednicas odontol\u00f3gicas.</p>"},{"location":"configuracao/ambiente/#variaveis-de-ambiente","title":"Vari\u00e1veis de Ambiente","text":"<p>O projeto utiliza vari\u00e1veis de ambiente para configura\u00e7\u00e3o flex\u00edvel. Copie o arquivo <code>.env.example</code> para <code>.env</code> e configure as seguintes vari\u00e1veis:</p>"},{"location":"configuracao/ambiente/#configuracoes-do-banco-de-dados","title":"Configura\u00e7\u00f5es do Banco de Dados","text":"<pre><code># Configura\u00e7\u00f5es do PostgreSQL\nDB_HOST=localhost\nDB_PORT=5432\nDB_NAME=clinica_odonto\nDB_USER=root\nDB_PASSWORD=root\n</code></pre> <p>Descri\u00e7\u00e3o das vari\u00e1veis: - <code>DB_HOST</code>: Endere\u00e7o do servidor PostgreSQL - <code>DB_PORT</code>: Porta do PostgreSQL (padr\u00e3o: 5432) - <code>DB_NAME</code>: Nome do banco de dados - <code>DB_USER</code>: Usu\u00e1rio do banco - <code>DB_PASSWORD</code>: Senha do usu\u00e1rio</p>"},{"location":"configuracao/ambiente/#configuracoes-do-azure-storage","title":"Configura\u00e7\u00f5es do Azure Storage","text":"<pre><code># Azure Storage Account Configuration\nAZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=seu_storage_account;AccountKey=sua_chave_de_acesso;EndpointSuffix=core.windows.net\n\n# Nome do container no Azure Storage\nAZURE_CONTAINER_NAME=landingzone\n</code></pre> <p>Descri\u00e7\u00e3o das vari\u00e1veis: - <code>AZURE_STORAGE_CONNECTION_STRING</code>: String de conex\u00e3o completa do Azure Storage - <code>AZURE_CONTAINER_NAME</code>: Nome do container onde os dados ser\u00e3o armazenados</p>"},{"location":"configuracao/ambiente/#obtendo-credenciais-do-azure","title":"Obtendo Credenciais do Azure","text":""},{"location":"configuracao/ambiente/#1-azure-storage-connection-string","title":"1. Azure Storage Connection String","text":"<ol> <li>Acesse o Portal Azure</li> <li>Navegue at\u00e9 sua Storage Account</li> <li>No menu lateral, clique em Access keys</li> <li>Copie a Connection string da Key1 ou Key2</li> </ol>"},{"location":"configuracao/ambiente/#2-container-name","title":"2. Container Name","text":"<p>O container ser\u00e1 criado automaticamente pelo script se n\u00e3o existir. Voc\u00ea pode usar: - <code>landingzone</code> - para dados brutos - <code>bronze</code> - para dados processados - <code>silver</code> - para dados limpos - <code>gold</code> - para dados agregados</p>"},{"location":"configuracao/ambiente/#estrutura-de-arquivos-de-configuracao","title":"Estrutura de Arquivos de Configura\u00e7\u00e3o","text":"<pre><code>.\n\u251c\u2500\u2500 .env                    # Suas configura\u00e7\u00f5es (n\u00e3o versionado)\n\u251c\u2500\u2500 .env.example           # Exemplo de configura\u00e7\u00e3o\n\u251c\u2500\u2500 docker/\n\u2502   \u2514\u2500\u2500 docker-compose.yml # Configura\u00e7\u00e3o do PostgreSQL\n\u2514\u2500\u2500 terraform/\n    \u251c\u2500\u2500 variables.tf       # Vari\u00e1veis do Terraform\n    \u2514\u2500\u2500 terraform.tfvars   # Valores das vari\u00e1veis (criar manualmente)\n</code></pre>"},{"location":"configuracao/ambiente/#validacao-da-configuracao","title":"Valida\u00e7\u00e3o da Configura\u00e7\u00e3o","text":"<p>Ap\u00f3s configurar as vari\u00e1veis de ambiente, voc\u00ea pode validar a configura\u00e7\u00e3o:</p> <pre><code># Testar conex\u00e3o com banco de dados\npython scripts/teste_db.py\n\n# Testar gera\u00e7\u00e3o de dados\npython scripts/gerador_dados.py\n</code></pre>"},{"location":"configuracao/ambiente/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuracao/ambiente/#erro-de-conexao-com-banco","title":"Erro de Conex\u00e3o com Banco","text":"<p>Se voc\u00ea receber erros de conex\u00e3o com o PostgreSQL:</p> <ol> <li> <p>Verifique se o Docker est\u00e1 rodando:    <pre><code>docker ps\n</code></pre></p> </li> <li> <p>Verifique os logs do container:    <pre><code>docker-compose -f docker/docker-compose.yml logs db\n</code></pre></p> </li> <li> <p>Teste a conex\u00e3o manualmente:    <pre><code>psql -h localhost -U root -d clinica_odonto\n</code></pre></p> </li> </ol>"},{"location":"configuracao/ambiente/#erro-de-azure-storage","title":"Erro de Azure Storage","text":"<p>Se voc\u00ea receber erros relacionados ao Azure Storage:</p> <ol> <li>Verifique se a connection string est\u00e1 correta</li> <li>Confirme se voc\u00ea tem permiss\u00f5es para criar containers</li> <li>Teste a conex\u00e3o usando Azure Storage Explorer</li> </ol>"},{"location":"configuracao/ambiente/#dependencias-python","title":"Depend\u00eancias Python","text":"<p>Se houver problemas com depend\u00eancias:</p> <pre><code># Reinstalar depend\u00eancias\npip install -r requirements.txt --force-reinstall\n\n# Verificar vers\u00f5es instaladas\npip list\n</code></pre>"},{"location":"configuracao/docker/","title":"Docker e Containeriza\u00e7\u00e3o","text":"<p>Este projeto utiliza Docker para simplificar a configura\u00e7\u00e3o do ambiente de desenvolvimento, especialmente para o banco de dados PostgreSQL.</p>"},{"location":"configuracao/docker/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>O arquivo <code>docker/docker-compose.yml</code> configura um container PostgreSQL pronto para uso com as seguintes caracter\u00edsticas:</p> <ul> <li>Imagem: PostgreSQL 15</li> <li>Container: <code>clinica_odonto</code></li> <li>Porta: 5432 (mapeada para o host)</li> <li>Credenciais: <code>root/root</code></li> <li>Banco: <code>clinica_odonto</code></li> </ul>"},{"location":"configuracao/docker/#configuracao-do-docker-compose","title":"Configura\u00e7\u00e3o do Docker Compose","text":"<pre><code>version: '3.8'\n\nservices:\n  db:\n    image: postgres:15\n    container_name: clinica_odonto\n    restart: always\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: clinica_odonto\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"configuracao/docker/#comandos-essenciais","title":"Comandos Essenciais","text":""},{"location":"configuracao/docker/#iniciar-os-servicos","title":"Iniciar os Servi\u00e7os","text":"<pre><code># Navegar at\u00e9 a pasta docker\ncd docker\n\n# Subir os containers em background\ndocker-compose up -d\n\n# Ver logs em tempo real\ndocker-compose logs -f\n</code></pre>"},{"location":"configuracao/docker/#gerenciar-os-servicos","title":"Gerenciar os Servi\u00e7os","text":"<pre><code># Parar os servi\u00e7os\ndocker-compose down\n\n# Reiniciar os servi\u00e7os\ndocker-compose restart\n\n# Ver status dos containers\ndocker-compose ps\n</code></pre>"},{"location":"configuracao/docker/#acesso-ao-banco-de-dados","title":"Acesso ao Banco de Dados","text":"<pre><code># Conectar via psql\npsql -h localhost -U root -d clinica_odonto\n\n# Executar comandos SQL direto\ndocker-compose exec db psql -U root -d clinica_odonto\n</code></pre>"},{"location":"configuracao/docker/#volumes-e-persistencia","title":"Volumes e Persist\u00eancia","text":"<p>O Docker Compose cria um volume nomeado <code>postgres_data</code> que persiste os dados do banco entre reinicializa\u00e7\u00f5es do container.</p>"},{"location":"configuracao/docker/#gerenciar-volumes","title":"Gerenciar Volumes","text":"<pre><code># Listar volumes\ndocker volume ls\n\n# Inspecionar o volume\ndocker volume inspect docker_postgres_data\n\n# Remover dados (cuidado!)\ndocker-compose down -v\n</code></pre>"},{"location":"configuracao/docker/#portas-e-networking","title":"Portas e Networking","text":"<ul> <li>PostgreSQL: Porta 5432 exposta no host</li> <li>Network: Rede padr\u00e3o do Docker Compose</li> <li>DNS: Os servi\u00e7os se comunicam pelo nome do servi\u00e7o</li> </ul>"},{"location":"configuracao/docker/#configuracoes-customizadas","title":"Configura\u00e7\u00f5es Customizadas","text":""},{"location":"configuracao/docker/#modificar-credenciais","title":"Modificar Credenciais","text":"<p>Para usar credenciais diferentes, edite o <code>docker-compose.yml</code>:</p> <pre><code>environment:\n  POSTGRES_USER: seu_usuario\n  POSTGRES_PASSWORD: sua_senha\n  POSTGRES_DB: seu_banco\n</code></pre>"},{"location":"configuracao/docker/#configuracoes-avancadas","title":"Configura\u00e7\u00f5es Avan\u00e7adas","text":"<p>Para configura\u00e7\u00f5es mais avan\u00e7adas do PostgreSQL:</p> <pre><code>services:\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_USER: root\n      POSTGRES_PASSWORD: root\n      POSTGRES_DB: clinica_odonto\n      POSTGRES_INITDB_ARGS: \"--encoding=UTF8 --locale=pt_BR.UTF-8\"\n    command: &gt;\n      postgres \n      -c shared_preload_libraries=pg_stat_statements\n      -c max_connections=200\n      -c shared_buffers=256MB\n</code></pre>"},{"location":"configuracao/docker/#monitoramento-e-logs","title":"Monitoramento e Logs","text":""},{"location":"configuracao/docker/#ver-logs","title":"Ver Logs","text":"<pre><code># Logs de todos os servi\u00e7os\ndocker-compose logs\n\n# Logs espec\u00edficos do banco\ndocker-compose logs db\n\n# Logs em tempo real\ndocker-compose logs -f db\n</code></pre>"},{"location":"configuracao/docker/#monitorar-performance","title":"Monitorar Performance","text":"<pre><code># Ver processos do container\ndocker-compose exec db ps aux\n\n# Ver conex\u00f5es ativas\ndocker-compose exec db psql -U root -d clinica_odonto -c \"SELECT * FROM pg_stat_activity;\"\n</code></pre>"},{"location":"configuracao/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuracao/docker/#container-nao-inicia","title":"Container n\u00e3o inicia","text":"<ol> <li> <p>Verificar se a porta 5432 est\u00e1 dispon\u00edvel:    <pre><code>netstat -an | grep 5432\n</code></pre></p> </li> <li> <p>Ver logs detalhados:    <pre><code>docker-compose logs db\n</code></pre></p> </li> <li> <p>Verificar recursos do sistema:    <pre><code>docker system df\n</code></pre></p> </li> </ol>"},{"location":"configuracao/docker/#problemas-de-conexao","title":"Problemas de Conex\u00e3o","text":"<ol> <li> <p>Verificar se o container est\u00e1 rodando:    <pre><code>docker-compose ps\n</code></pre></p> </li> <li> <p>Testar conectividade:    <pre><code>telnet localhost 5432\n</code></pre></p> </li> <li> <p>Verificar configura\u00e7\u00f5es de rede:    <pre><code>docker network ls\ndocker network inspect docker_default\n</code></pre></p> </li> </ol>"},{"location":"configuracao/docker/#reset-completo","title":"Reset Completo","text":"<p>Se necess\u00e1rio, voc\u00ea pode fazer um reset completo:</p> <pre><code># Parar e remover containers e volumes\ndocker-compose down -v\n\n# Remover imagens (opcional)\ndocker rmi postgres:15\n\n# Recriar tudo\ndocker-compose up -d\n</code></pre>"},{"location":"configuracao/docker/#integracao-com-o-pipeline-etl","title":"Integra\u00e7\u00e3o com o Pipeline ETL","text":"<p>O banco PostgreSQL no Docker integra perfeitamente com o pipeline ETL:</p> <ol> <li>Gera\u00e7\u00e3o de Dados: O <code>gerador_dados.py</code> se conecta automaticamente</li> <li>Notebooks: Os Jupyter notebooks usam as mesmas credenciais</li> <li>Scripts SQL: Podem ser executados diretamente no container</li> </ol>"},{"location":"configuracao/docker/#exemplo-de-uso","title":"Exemplo de Uso","text":"<pre><code># 1. Subir o banco\ndocker-compose -f docker/docker-compose.yml up -d\n\n# 2. Executar scripts SQL\npsql -h localhost -U root -d clinica_odonto -f scripts/modelo_fisico.sql\n\n# 3. Gerar dados\npython scripts/gerador_dados.py\n\n# 4. Executar notebooks\njupyter lab\n</code></pre>"},{"location":"configuracao/docker/#alternativas-ao-docker","title":"Alternativas ao Docker","text":"<p>Se preferir n\u00e3o usar Docker:</p> <ol> <li>PostgreSQL Local: Instale PostgreSQL nativamente</li> <li>PostgreSQL na Nuvem: Use servi\u00e7os como Azure Database for PostgreSQL</li> <li>SQLite: Para desenvolvimento simples (requer modifica\u00e7\u00f5es no c\u00f3digo)</li> </ol>"},{"location":"configuracao/terraform/","title":"Terraform e Azure","text":"<p>Este projeto inclui configura\u00e7\u00f5es Terraform para provisionar infraestrutura na Azure, criando um ambiente completo de Data Lake com Azure Storage e Azure Databricks.</p>"},{"location":"configuracao/terraform/#visao-geral-da-infraestrutura","title":"Vis\u00e3o Geral da Infraestrutura","text":"<p>O Terraform cria os seguintes recursos na Azure:</p>"},{"location":"configuracao/terraform/#recursos-principais","title":"Recursos Principais","text":"<ul> <li>Resource Group: <code>rgdata{company}{env}01</code></li> <li>Storage Account: <code>stacdata{company}{env}01</code></li> <li>Storage Containers: <code>landingzone</code>, <code>bronze</code>, <code>silver</code>, <code>gold</code></li> <li>Azure Databricks Workspace: Para processamento de big data</li> <li>Key Vault: Para gerenciamento seguro de credenciais</li> </ul>"},{"location":"configuracao/terraform/#organizacao-por-camadas","title":"Organiza\u00e7\u00e3o por Camadas","text":"<pre><code>Azure Storage Account\n\u251c\u2500\u2500 landingzone/     # Dados brutos ingeridos\n\u251c\u2500\u2500 bronze/          # Dados estruturados\n\u251c\u2500\u2500 silver/          # Dados limpos e validados\n\u2514\u2500\u2500 gold/            # Dados agregados e prontos para an\u00e1lise\n</code></pre>"},{"location":"configuracao/terraform/#arquivos-terraform","title":"Arquivos Terraform","text":""},{"location":"configuracao/terraform/#maintf","title":"main.tf","text":"<p>Cont\u00e9m a defini\u00e7\u00e3o dos recursos principais:</p> <pre><code># Resource Group\nresource \"azurerm_resource_group\" \"rgdata01\" {\n  name     = \"rgdata${var.company}${var.env}01\"\n  location = var.default_location\n}\n\n# Storage Account\nresource \"azurerm_storage_account\" \"stacdata01\" {\n  name                     = local.stgaccname\n  resource_group_name      = azurerm_resource_group.rgdata01.name\n  location                 = azurerm_resource_group.rgdata01.location\n  account_tier             = \"Standard\"\n  account_replication_type = \"LRS\"\n  is_hns_enabled           = true\n}\n</code></pre>"},{"location":"configuracao/terraform/#variablestf","title":"variables.tf","text":"<p>Define as vari\u00e1veis configur\u00e1veis:</p> <pre><code>variable \"env\" {\n  type    = string\n  default = \"\"\n  description = \"Ambiente (dev, prod, etc.)\"\n}\n\nvariable \"company\" {\n  default = \"trabalhoed\"\n  type    = string\n  description = \"Nome da empresa/projeto\"\n}\n\nvariable \"default_location\" {\n  default = \"Brazil South\"\n  type    = string\n  description = \"Regi\u00e3o do Azure\"\n}\n</code></pre>"},{"location":"configuracao/terraform/#providerstf","title":"providers.tf","text":"<p>Configura os provedores necess\u00e1rios:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt;3.0\"\n    }\n    azuread = {\n      source  = \"hashicorp/azuread\"\n      version = \"~&gt;2.0\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n</code></pre>"},{"location":"configuracao/terraform/#configuracao-e-deploy","title":"Configura\u00e7\u00e3o e Deploy","text":""},{"location":"configuracao/terraform/#1-pre-requisitos","title":"1. Pr\u00e9-requisitos","text":"<pre><code># Instalar Azure CLI\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n\n# Instalar Terraform\nwget https://releases.hashicorp.com/terraform/1.6.0/terraform_1.6.0_linux_amd64.zip\nunzip terraform_1.6.0_linux_amd64.zip\nsudo mv terraform /usr/local/bin/\n\n# Fazer login na Azure\naz login\n</code></pre>"},{"location":"configuracao/terraform/#2-configurar-variaveis","title":"2. Configurar Vari\u00e1veis","text":"<p>Crie um arquivo <code>terraform.tfvars</code>:</p> <pre><code>env              = \"dev\"\ncompany          = \"suaempresa\"\ndefault_location = \"Brazil South\"\n</code></pre>"},{"location":"configuracao/terraform/#3-executar-terraform","title":"3. Executar Terraform","text":"<pre><code># Navegar para a pasta terraform\ncd terraform\n\n# Inicializar Terraform\nterraform init\n\n# Validar configura\u00e7\u00e3o\nterraform validate\n\n# Ver plano de execu\u00e7\u00e3o\nterraform plan\n\n# Aplicar mudan\u00e7as\nterraform apply\n</code></pre>"},{"location":"configuracao/terraform/#recursos-criados","title":"Recursos Criados","text":""},{"location":"configuracao/terraform/#storage-account","title":"Storage Account","text":"<ul> <li>Nome: <code>stacdatatrabalhoeddev01</code> (exemplo)</li> <li>Tipo: Standard LRS</li> <li>Hierarquia: Habilitada (Data Lake Gen2)</li> <li>Containers: landingzone, bronze, silver, gold</li> </ul>"},{"location":"configuracao/terraform/#azure-databricks","title":"Azure Databricks","text":"<ul> <li>SKU: Standard</li> <li>Managed Resource Group: Criado automaticamente</li> <li>Integra\u00e7\u00e3o: Key Vault para secrets</li> </ul>"},{"location":"configuracao/terraform/#key-vault","title":"Key Vault","text":"<ul> <li>Policies: Configuradas para Databricks</li> <li>Secrets: Conex\u00f5es de banco e storage</li> <li>Acesso: Baseado em roles</li> </ul>"},{"location":"configuracao/terraform/#configuracao-de-secrets","title":"Configura\u00e7\u00e3o de Secrets","text":""},{"location":"configuracao/terraform/#connection-strings","title":"Connection Strings","text":"<p>O Terraform configura automaticamente os secrets necess\u00e1rios:</p> <pre><code>resource \"azurerm_key_vault_secret\" \"storage_connection\" {\n  name         = \"storage-connection-string\"\n  value        = azurerm_storage_account.stacdata01.primary_connection_string\n  key_vault_id = azurerm_key_vault.kvdata01.id\n}\n</code></pre>"},{"location":"configuracao/terraform/#databricks-secrets","title":"Databricks Secrets","text":"<pre><code># Configurar CLI do Databricks\ndatabricks configure --token\n\n# Criar scope de secrets\ndatabricks secrets create-scope --scope \"azkvscope\"\n\n# Listar secrets\ndatabricks secrets list --scope \"azkvscope\"\n</code></pre>"},{"location":"configuracao/terraform/#customizacao","title":"Customiza\u00e7\u00e3o","text":""},{"location":"configuracao/terraform/#ambientes-multiplos","title":"Ambientes M\u00faltiplos","text":"<p>Para criar ambientes separados:</p> <pre><code># Desenvolvimento\nterraform workspace new dev\nterraform apply -var=\"env=dev\"\n\n# Produ\u00e7\u00e3o\nterraform workspace new prod\nterraform apply -var=\"env=prod\"\n</code></pre>"},{"location":"configuracao/terraform/#configuracoes-regionais","title":"Configura\u00e7\u00f5es Regionais","text":"<p>Para outras regi\u00f5es:</p> <pre><code># terraform.tfvars\ndefault_location = \"East US\"\n# ou\ndefault_location = \"West Europe\"\n</code></pre>"},{"location":"configuracao/terraform/#monitoramento-e-gestao","title":"Monitoramento e Gest\u00e3o","text":""},{"location":"configuracao/terraform/#outputs-importantes","title":"Outputs Importantes","text":"<p>O Terraform exp\u00f5e informa\u00e7\u00f5es \u00fateis:</p> <pre><code>output \"storage_account_name\" {\n  value = azurerm_storage_account.stacdata01.name\n}\n\noutput \"storage_connection_string\" {\n  value     = azurerm_storage_account.stacdata01.primary_connection_string\n  sensitive = true\n}\n\noutput \"databricks_workspace_url\" {\n  value = azurerm_databricks_workspace.dbw01.workspace_url\n}\n</code></pre>"},{"location":"configuracao/terraform/#ver-outputs","title":"Ver Outputs","text":"<pre><code># Ver todos os outputs\nterraform output\n\n# Ver output espec\u00edfico\nterraform output storage_account_name\n\n# Ver outputs sens\u00edveis\nterraform output -raw storage_connection_string\n</code></pre>"},{"location":"configuracao/terraform/#integracao-com-o-pipeline-etl","title":"Integra\u00e7\u00e3o com o Pipeline ETL","text":""},{"location":"configuracao/terraform/#atualizar-env","title":"Atualizar .env","text":"<p>Ap\u00f3s o deploy, atualize seu <code>.env</code>:</p> <pre><code># Usar outputs do Terraform\nexport STORAGE_NAME=$(terraform output -raw storage_account_name)\nexport CONNECTION_STRING=$(terraform output -raw storage_connection_string)\n\n# Atualizar .env\necho \"AZURE_STORAGE_CONNECTION_STRING=$CONNECTION_STRING\" &gt;&gt; .env\necho \"AZURE_CONTAINER_NAME=landingzone\" &gt;&gt; .env\n</code></pre>"},{"location":"configuracao/terraform/#configurar-databricks","title":"Configurar Databricks","text":"<ol> <li>Acesse o workspace do Databricks</li> <li>Configure clusters para processamento</li> <li>Import notebooks do projeto</li> <li>Configure secrets para conex\u00f5es</li> </ol>"},{"location":"configuracao/terraform/#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuracao/terraform/#erro-de-autenticacao","title":"Erro de Autentica\u00e7\u00e3o","text":"<pre><code># Verificar login\naz account show\n\n# Refazer login se necess\u00e1rio\naz login --tenant YOUR_TENANT_ID\n</code></pre>"},{"location":"configuracao/terraform/#erro-de-recursos","title":"Erro de Recursos","text":"<pre><code># Verificar quotas da subscription\naz vm list-usage --location \"Brazil South\" -o table\n\n# Verificar providers registrados\naz provider list --query \"[?registrationState=='Registered']\" -o table\n</code></pre>"},{"location":"configuracao/terraform/#rollback","title":"Rollback","text":"<pre><code># Ver hist\u00f3rico de state\nterraform show\n\n# Rollback espec\u00edfico (cuidado!)\nterraform import azurerm_storage_account.stacdata01 /subscriptions/.../resourceGroups/.../providers/Microsoft.Storage/storageAccounts/...\n</code></pre>"},{"location":"configuracao/terraform/#limpeza-de-recursos","title":"Limpeza de Recursos","text":""},{"location":"configuracao/terraform/#destruir-infraestrutura","title":"Destruir Infraestrutura","text":"<pre><code># Ver o que ser\u00e1 destru\u00eddo\nterraform plan -destroy\n\n# Destruir recursos\nterraform destroy\n\n# Confirmar destrui\u00e7\u00e3o\n# Digite \"yes\" quando solicitado\n</code></pre>"},{"location":"configuracao/terraform/#limpeza-manual","title":"Limpeza Manual","text":"<p>Se necess\u00e1rio, remova recursos manualmente:</p> <pre><code># Listar resource groups\naz group list -o table\n\n# Deletar resource group (remove todos os recursos)\naz group delete --name rgdatatrabalhoeddev01 --yes --no-wait\n</code></pre>"},{"location":"configuracao/terraform/#melhores-praticas","title":"Melhores Pr\u00e1ticas","text":"<ol> <li>Versionamento: Use tags Git para versionar infraestrutura</li> <li>State Backend: Configure backend remoto para produ\u00e7\u00e3o</li> <li>Terraform Cloud: Use para equipes maiores</li> <li>Modules: Modularize c\u00f3digo para reutiliza\u00e7\u00e3o</li> <li>Security: Nunca commite <code>terraform.tfvars</code> com dados sens\u00edveis</li> </ol>"},{"location":"introducao/arquitetura/","title":"Arquitetura do Pipeline ETL","text":"<p>A arquitetura do nosso pipeline ETL para dados de cl\u00ednicas odontol\u00f3gicas segue o padr\u00e3o de Data Lake com arquitetura medalh\u00e3o. Este modelo \u00e9 fundamental para garantir a organiza\u00e7\u00e3o, a qualidade e a rastreabilidade dos dados em diferentes est\u00e1gios de processamento, desde a sua origem at\u00e9 a camada de consumo.</p> <p></p> <p>Componentes Chave da Arquitetura:</p> <ol> <li> <p>Fontes de Dados (Simuladas):</p> <ul> <li>Atualmente, os dados s\u00e3o gerados programaticamente utilizando Python (via <code>scripts/gerador_dados.py</code>) e armazenados em arquivos CSV.</li> <li>Esses arquivos CSV representam diversas entidades relacionadas a opera\u00e7\u00f5es de cl\u00ednicas, como pacientes, odontologistas, consultas, procedimentos, pagamentos, etc.</li> </ul> </li> <li> <p>Camada Landing (Raw):</p> <ul> <li>Localiza\u00e7\u00e3o: <code>data/raw/</code></li> <li>Prop\u00f3sito: Esta \u00e9 a \u00e1rea de recep\u00e7\u00e3o inicial dos dados. Os arquivos CSV gerados s\u00e3o colocados diretamente aqui, sem nenhuma transforma\u00e7\u00e3o ou valida\u00e7\u00e3o de conte\u00fado.</li> <li>Caracter\u00edsticas: Preserva a integridade e o formato original dos dados, servindo como uma fonte imut\u00e1vel e audit\u00e1vel.</li> </ul> </li> <li> <p>Camada Bronze:</p> <ul> <li>Localiza\u00e7\u00e3o: <code>data/bronze/</code></li> <li>Prop\u00f3sito: Recebe os dados da camada Landing ap\u00f3s uma ingest\u00e3o inicial. Nesta etapa, os dados s\u00e3o lidos, validados em termos de formato b\u00e1sico (se s\u00e3o arquivos CSV v\u00e1lidos, por exemplo) e tipagem, e salvos novamente.</li> <li>Ferramenta: <code>notebooks/notebook_landing_bronze.ipynb</code></li> <li>Caracter\u00edsticas: Os dados ainda s\u00e3o brutos em sua ess\u00eancia, mas j\u00e1 passaram por um controle m\u00ednimo de qualidade para garantir que s\u00e3o leg\u00edveis e estruturados para as pr\u00f3ximas etapas.</li> </ul> </li> <li> <p>Camada Silver:</p> <ul> <li>Localiza\u00e7\u00e3o: <code>data/silver/</code></li> <li>Prop\u00f3sito: Esta camada \u00e9 onde ocorrem as principais transforma\u00e7\u00f5es e limpezas dos dados. Dados s\u00e3o padronizados, inconsist\u00eancias s\u00e3o tratadas, valores nulos s\u00e3o manipulados e regras de neg\u00f3cio s\u00e3o aplicadas.</li> <li>Ferramenta: <code>notebooks/notebook_bronze_silver.ipynb</code></li> <li>Caracter\u00edsticas: Os dados est\u00e3o limpos, padronizados e prontos para serem usados em an\u00e1lises mais complexas ou para serem modelados.</li> </ul> </li> <li> <p>Camada Gold (Data Warehouse):</p> <ul> <li>Localiza\u00e7\u00e3o: <code>data/gold/</code> (para arquivos intermedi\u00e1rios, se houver) e Banco de Dados Relacional (PostgreSQL) para consumo.</li> <li>Prop\u00f3sito: A camada final do pipeline. Aqui, os dados limpos da camada Silver s\u00e3o agregados e modelados em um formato dimensional (estrela ou floco de neve), otimizado para consultas anal\u00edticas e ferramentas de Business Intelligence (BI).</li> <li>Ferramenta: <code>notebooks/notebook_silver_gold.ipynb</code> e scripts SQL (<code>scripts/modelo_fisico.sql</code>, <code>scripts/modelo_dimensional.sql</code>).</li> <li>Caracter\u00edsticas: Dados consolidados, de alta qualidade, estruturados em tabelas de fato e dimens\u00e3o, prontos para gerar insights de neg\u00f3cio e alimentar dashboards.</li> </ul> </li> </ol> <p>Este fluxo garante um processo robusto de ETL, onde cada camada adiciona valor e refinamento aos dados, culminando em um Data Warehouse confi\u00e1vel para a tomada de decis\u00f5es.</p>"},{"location":"introducao/visao_geral/","title":"Vis\u00e3o Geral do Projeto","text":"<p>Este projeto foi desenvolvido como parte da disciplina de Engenharia de Dados do curso de Engenharia de Software da UNISATC. Ele simula um cen\u00e1rio real de ingest\u00e3o, transforma\u00e7\u00e3o e carregamento de dados para um sistema de gest\u00e3o de cl\u00ednicas odontol\u00f3gicas. O objetivo \u00e9 demonstrar a aplica\u00e7\u00e3o de conceitos e pr\u00e1ticas de engenharia de dados, desde a coleta de dados brutos at\u00e9 a disponibiliza\u00e7\u00e3o de informa\u00e7\u00f5es prontas para an\u00e1lise.</p> <p>Objetivos Principais:</p> <ul> <li>Ingest\u00e3o de Dados: Realizar a coleta de dados de diversas fontes (atualmente arquivos CSV simulados) para uma \u00e1rea de landing zone.</li> <li>Pipeline de Transforma\u00e7\u00e3o: Implementar um pipeline de ETL (Extract, Transform, Load) utilizando a arquitetura medalh\u00e3o (Landing, Bronze, Silver, Gold). Isso garante a progressiva limpeza, enriquecimento e estrutura\u00e7\u00e3o dos dados.</li> <li>Modelagem Dimensional: Modelar os dados finais em um formato dimensional (Data Warehouse), otimizado para facilitar a an\u00e1lise de neg\u00f3cios e a cria\u00e7\u00e3o de dashboards.</li> <li>Ferramentas e Tecnologias: Utilizar ferramentas e linguagens de programa\u00e7\u00e3o comuns no ecossistema de engenharia de dados, como Python, Pandas e SQL, para manipular e transformar os conjuntos de dados.</li> <li>Documenta\u00e7\u00e3o: Fornecer uma documenta\u00e7\u00e3o clara e abrangente do pipeline, sua arquitetura e as transforma\u00e7\u00f5es aplicadas, utilizando MkDocs.</li> </ul> <p>Este projeto busca ser uma demonstra\u00e7\u00e3o pr\u00e1tica de como dados de diferentes fontes podem ser integrados e transformados para gerar insights valiosos para a gest\u00e3o de uma cl\u00ednica odontol\u00f3gica.</p>"},{"location":"modelagem/modelo_dimensional/","title":"Modelo Dimensional (Data Warehouse)","text":"<p>O modelo dimensional \u00e9 a estrutura final dos dados na camada Gold, projetada especificamente para an\u00e1lise de neg\u00f3cios e gera\u00e7\u00e3o de relat\u00f3rios. Ele adota o padr\u00e3o de Esquema Estrela (Star Schema), que \u00e9 otimizado para consultas de agrega\u00e7\u00e3o e facilidade de entendimento por usu\u00e1rios de neg\u00f3cio.</p> <p>Prop\u00f3sito:</p> <ul> <li>An\u00e1lise Simplificada: Oferecer uma estrutura de dados intuitiva que permite aos analistas de neg\u00f3cio consultar facilmente as informa\u00e7\u00f5es sem a necessidade de entender complexidades de normaliza\u00e7\u00e3o.</li> <li>Desempenho de Consulta: Otimizar o desempenho de consultas agregadas, que s\u00e3o comuns em ferramentas de Business Intelligence (BI).</li> <li>Flexibilidade: Permitir a adi\u00e7\u00e3o de novas m\u00e9tricas e atributos dimensionais com impacto m\u00ednimo na estrutura existente.</li> </ul> <p>Vis\u00e3o Geral do Esquema Estrela:</p> <p>O Esquema Estrela consiste em uma Tabela de Fato central e v\u00e1rias Tabelas de Dimens\u00e3o que se conectam diretamente a ela.</p> <ul> <li>Tabelas de Fato: Cont\u00eam as m\u00e9tricas quantitativas (o \"o qu\u00ea\" aconteceu) e as chaves estrangeiras que apontam para as tabelas de dimens\u00e3o.</li> <li>Tabelas de Dimens\u00e3o: Cont\u00eam os atributos descritivos (o \"quem\", \"quando\", \"onde\", \"como\") que fornecem contexto \u00e0s m\u00e9tricas da tabela de fato.</li> </ul> <p>Detalhes das Tabelas do Modelo Dimensional:</p>"},{"location":"modelagem/modelo_dimensional/#tabela-de-fato-fato_consultas_e_pagamentos","title":"Tabela de Fato: <code>fato_consultas_e_pagamentos</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Esta \u00e9 a tabela de fato principal que captura os eventos de consultas e os pagamentos associados. Ela cont\u00e9m as m\u00e9tricas de neg\u00f3cio e as chaves estrangeiras para as dimens\u00f5es que fornecem o contexto desses eventos.</li> <li>Granularidade: Uma linha por evento de consulta/pagamento.</li> <li>M\u00e9tricas (Exemplos):<ul> <li><code>valor_total_pago</code>: O valor monet\u00e1rio total transacionado.</li> <li><code>quantidade_procedimentos</code>: O n\u00famero de procedimentos realizados em uma consulta.</li> <li><code>duracao_minutos</code>: A dura\u00e7\u00e3o estimada da consulta em minutos.</li> </ul> </li> <li>Chaves Estrangeiras (para Dimens\u00f5es):<ul> <li><code>id_tempo_sk</code>: Chave para a <code>dim_tempo</code>.</li> <li><code>id_paciente_sk</code>: Chave para a <code>dim_paciente</code>.</li> <li><code>id_odontologista_sk</code>: Chave para a <code>dim_odontologista</code>.</li> <li><code>id_procedimento_sk</code>: Chave para a <code>dim_procedimento</code>.</li> <li><code>id_tipo_pagamento_sk</code>: Chave para a <code>dim_tipo_pagamento</code>.</li> </ul> </li> </ul>"},{"location":"modelagem/modelo_dimensional/#dimensoes","title":"Dimens\u00f5es:","text":"<p>As tabelas de dimens\u00e3o fornecem o contexto para as m\u00e9tricas da tabela de fato, permitindo a an\u00e1lise por diferentes perspectivas.</p>"},{"location":"modelagem/modelo_dimensional/#1-dim_paciente","title":"1. <code>dim_paciente</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Detalhes dos pacientes.</li> <li>Atributos (Exemplos): <code>nome_paciente</code>, <code>data_nascimento</code>, <code>genero</code>, <code>telefone</code>, <code>email</code>, <code>endereco_completo</code>, <code>idade</code>, etc.</li> </ul>"},{"location":"modelagem/modelo_dimensional/#2-dim_odontologista","title":"2. <code>dim_odontologista</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Detalhes dos profissionais odontologistas.</li> <li>Atributos (Exemplos): <code>nome_odontologista</code>, <code>especialidade</code>, <code>CRO</code>, etc.</li> </ul>"},{"location":"modelagem/modelo_dimensional/#3-dim_procedimento","title":"3. <code>dim_procedimento</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Informa\u00e7\u00f5es sobre os procedimentos oferecidos pela cl\u00ednica.</li> <li>Atributos (Exemplos): <code>nome_procedimento</code>, <code>descricao_procedimento</code>, <code>custo_base</code>, etc.</li> </ul>"},{"location":"modelagem/modelo_dimensional/#4-dim_tipo_pagamento","title":"4. <code>dim_tipo_pagamento</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Detalhes sobre os tipos de pagamento.</li> <li>Atributos (Exemplos): <code>descricao_tipo_pagamento</code>.</li> </ul>"},{"location":"modelagem/modelo_dimensional/#5-dim_tempo","title":"5. <code>dim_tempo</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Dimens\u00e3o de tempo para an\u00e1lise temporal de eventos.</li> <li>Atributos (Exemplos): <code>data_completa</code>, <code>ano</code>, <code>mes</code>, <code>nome_mes</code>, <code>dia_do_mes</code>, <code>dia_da_semana</code>, <code>trimestre</code>, <code>semestre</code>, <code>feriado</code>, etc. (Permite fatiar e agregar dados por qualquer per\u00edodo de tempo).</li> </ul> <p>Benef\u00edcios do Modelo Dimensional:</p> <ul> <li>Performance: Consultas que envolvem agrega\u00e7\u00f5es e filtros s\u00e3o muito mais r\u00e1pidas devido \u00e0 desnormaliza\u00e7\u00e3o e menor n\u00famero de joins complexos.</li> <li>Simplicidade: O modelo \u00e9 intuitivo e f\u00e1cil de entender, o que empodera os usu\u00e1rios de neg\u00f3cio a fazerem suas pr\u00f3prias an\u00e1lises com menor depend\u00eancia de TI.</li> <li>Manutenibilidade: Adicionar novas m\u00e9tricas ou atributos a dimens\u00f5es existentes geralmente n\u00e3o requer mudan\u00e7as dr\u00e1sticas no modelo.</li> </ul> <p>Este modelo representa a forma final como os dados s\u00e3o apresentados para consumo, tornando a an\u00e1lise de dados sobre as opera\u00e7\u00f5es da cl\u00ednica odontol\u00f3gica eficiente e acess\u00edvel.</p>"},{"location":"modelagem/modelo_fisico/","title":"Modelo F\u00edsico do Banco de Dados","text":"<p>O modelo f\u00edsico do banco de dados descreve a implementa\u00e7\u00e3o real das tabelas, colunas, tipos de dados, chaves prim\u00e1rias e chaves estrangeiras que comp\u00f5em o nosso Data Warehouse na camada Gold. Este modelo \u00e9 criado e gerenciado pelo script SQL <code>scripts/modelo_fisico.sql</code>.</p> <p>Prop\u00f3sito:</p> <ul> <li>Estrutura da Base de Dados: Definir a estrutura exata das tabelas que armazenar\u00e3o os dados finais, garantindo a integridade e a consist\u00eancia dos dados.</li> <li>Otimiza\u00e7\u00e3o: Estabelecer \u00edndices e restri\u00e7\u00f5es que otimizem o desempenho das consultas anal\u00edticas e a integridade referencial.</li> <li>Base para Carregamento: Fornecer a estrutura necess\u00e1ria para que o pipeline ETL (<code>notebook_silver_gold.ipynb</code>) possa carregar os dados transformados.</li> </ul> <p>Ferramenta Utilizada:</p> <ul> <li>Script SQL: <code>scripts/modelo_fisico.sql</code> (executado no PostgreSQL ou outro SGBD relacional).</li> </ul> <p>Vis\u00e3o Geral das Tabelas Criadas:</p> <p>O script <code>modelo_fisico.sql</code> \u00e9 respons\u00e1vel por criar as tabelas que servir\u00e3o de base para as dimens\u00f5es e fatos do nosso modelo dimensional. Embora a camada Gold use um modelo dimensional, as tabelas f\u00edsicas podem ser criadas com base na estrutura final que receber\u00e1 os dados.</p> <p>Abaixo, descrevemos a estrutura prov\u00e1vel das tabelas que este script cria, com base nas entidades do seu projeto:</p>"},{"location":"modelagem/modelo_fisico/#tabela-dim_paciente","title":"Tabela <code>dim_paciente</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Armazena informa\u00e7\u00f5es detalhadas sobre os pacientes da cl\u00ednica.</li> <li>Colunas:<ul> <li><code>id_paciente</code> (PRIMARY KEY): Identificador \u00fanico do paciente.</li> <li><code>nome_paciente</code>: Nome completo do paciente.</li> <li><code>data_nascimento</code>: Data de nascimento do paciente.</li> <li><code>genero</code>: G\u00eanero do paciente.</li> <li><code>telefone</code>: N\u00famero de telefone de contato.</li> <li><code>email</code>: Endere\u00e7o de e-mail.</li> <li><code>endereco_completo</code>: Endere\u00e7o combinado (rua, n\u00famero, bairro, cidade, estado, CEP).</li> <li><code>idade</code>: Idade calculada do paciente.</li> <li>(Outras colunas relevantes do paciente)</li> </ul> </li> </ul>"},{"location":"modelagem/modelo_fisico/#tabela-dim_odontologista","title":"Tabela <code>dim_odontologista</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Cont\u00e9m os dados dos profissionais odontologistas.</li> <li>Colunas:<ul> <li><code>id_odontologista</code> (PRIMARY KEY): Identificador \u00fanico do odontologista.</li> <li><code>nome_odontologista</code>: Nome completo do odontologista.</li> <li><code>especialidade</code>: Especialidade do profissional.</li> <li><code>CRO</code>: N\u00famero de registro no Conselho Regional de Odontologia.</li> <li>(Outras colunas relevantes do odontologista)</li> </ul> </li> </ul>"},{"location":"modelagem/modelo_fisico/#tabela-dim_procedimento","title":"Tabela <code>dim_procedimento</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Detalhes sobre os procedimentos odontol\u00f3gicos oferecidos.</li> <li>Colunas:<ul> <li><code>id_procedimento</code> (PRIMARY KEY): Identificador \u00fanico do procedimento.</li> <li><code>nome_procedimento</code>: Nome descritivo do procedimento.</li> <li><code>descricao_procedimento</code>: Descri\u00e7\u00e3o detalhada do procedimento.</li> <li><code>custo_base</code>: Custo padr\u00e3o do procedimento.</li> <li>(Outras colunas relevantes do procedimento)</li> </ul> </li> </ul>"},{"location":"modelagem/modelo_fisico/#tabela-dim_tipo_pagamento","title":"Tabela <code>dim_tipo_pagamento</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Detalhes sobre os m\u00e9todos de pagamento.</li> <li>Colunas:<ul> <li><code>id_tipo_pagamento</code> (PRIMARY KEY): Identificador \u00fanico do tipo de pagamento.</li> <li><code>descricao_tipo_pagamento</code>: Descri\u00e7\u00e3o do tipo de pagamento (ex: \"Cart\u00e3o de Cr\u00e9dito\", \"Dinheiro\", \"Plano de Sa\u00fade\").</li> </ul> </li> </ul>"},{"location":"modelagem/modelo_fisico/#tabela-dim_tempo","title":"Tabela <code>dim_tempo</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Tabela de dimens\u00e3o de tempo para an\u00e1lises baseadas em per\u00edodo.</li> <li>Colunas:<ul> <li><code>data_sk</code> (PRIMARY KEY): Chave suplicada para a data (ex: YYYYMMDD).</li> <li><code>data_completa</code>: Data no formato <code>YYYY-MM-DD</code>.</li> <li><code>ano</code>: Ano da data.</li> <li><code>mes</code>: N\u00famero do m\u00eas (1-12).</li> <li><code>nome_mes</code>: Nome do m\u00eas (ex: \"Janeiro\").</li> <li><code>dia_do_mes</code>: Dia do m\u00eas.</li> <li><code>dia_da_semana</code>: Dia da semana (ex: \"Segunda-feira\").</li> <li><code>trimestre</code>: Trimestre do ano.</li> <li><code>semestre</code>: Semestre do ano.</li> <li><code>feriado</code>: Indica se a data \u00e9 um feriado (booleano/texto).</li> <li>(Outras colunas relevantes de tempo)</li> </ul> </li> </ul>"},{"location":"modelagem/modelo_fisico/#tabela-fato_consultas_e_pagamentos","title":"Tabela <code>fato_consultas_e_pagamentos</code>","text":"<ul> <li>Descri\u00e7\u00e3o: Tabela de fato central que registra eventos de consultas e pagamentos, com m\u00e9tricas e chaves para as dimens\u00f5es.</li> <li>Colunas:<ul> <li><code>id_consulta_fato</code> (PRIMARY KEY): Identificador \u00fanico da linha de fato (pode ser gerado).</li> <li><code>id_consulta_origem</code>: ID da consulta na origem (para rastreabilidade).</li> <li><code>data_sk</code> (FOREIGN KEY para <code>dim_tempo</code>): Chave da dimens\u00e3o tempo.</li> <li><code>id_paciente_sk</code> (FOREIGN KEY para <code>dim_paciente</code>): Chave da dimens\u00e3o paciente.</li> <li><code>id_odontologista_sk</code> (FOREIGN KEY para <code>dim_odontologista</code>): Chave da dimens\u00e3o odontologista.</li> <li><code>id_procedimento_sk</code> (FOREIGN KEY para <code>dim_procedimento</code>): Chave da dimens\u00e3o procedimento.</li> <li><code>id_tipo_pagamento_sk</code> (FOREIGN KEY para <code>dim_tipo_pagamento</code>): Chave da dimens\u00e3o tipo de pagamento.</li> <li><code>valor_total_pago</code>: M\u00e9trica: Valor total pago pela consulta/procedimento.</li> <li><code>quantidade_procedimentos</code>: M\u00e9trica: N\u00famero de procedimentos realizados na consulta.</li> <li><code>duracao_minutos</code>: M\u00e9trica: Dura\u00e7\u00e3o da consulta em minutos.</li> <li>(Outras m\u00e9tricas e chaves estrangeiras relevantes)</li> </ul> </li> </ul> <p>Exemplo de SQL (Estrutura B\u00e1sica):</p> <p>```sql -- Exemplo de cria\u00e7\u00e3o de tabela dim_paciente CREATE TABLE IF NOT EXISTS dim_paciente (     id_paciente_sk SERIAL PRIMARY KEY,     id_paciente VARCHAR(50) UNIQUE, -- ID da fonte original     nome_paciente VARCHAR(255),     data_nascimento DATE,     genero VARCHAR(10),     telefone VARCHAR(20),     email VARCHAR(100),     endereco_completo TEXT,     idade INT );</p> <p>-- Exemplo de cria\u00e7\u00e3o de tabela fato_consultas_e_pagamentos CREATE TABLE IF NOT EXISTS fato_consultas_e_pagamentos (     id_consulta_fato SERIAL PRIMARY KEY,     id_consulta_origem VARCHAR(50),     data_sk INT,     id_paciente_sk INT,     id_odontologista_sk INT,     id_procedimento_sk INT,     id_tipo_pagamento_sk INT,     valor_total_pago NUMERIC(10, 2),     quantidade_procedimentos INT,     duracao_minutos INT,     FOREIGN KEY (data_sk) REFERENCES dim_tempo(data_sk),     FOREIGN KEY (id_paciente_sk) REFERENCES dim_paciente(id_paciente_sk),     FOREIGN KEY (id_odontologista_sk) REFERENCES dim_odontologista(id_odontologista_sk),     FOREIGN KEY (id_procedimento_sk) REFERENCES dim_procedimento(id_procedimento_sk),     FOREIGN KEY (id_tipo_pagamento_sk) REFERENCES dim_tipo_pagamento(id_tipo_pagamento_sk) ); -- ... (outras tabelas dim e fato)</p>"},{"location":"pipeline/geracao_dados/","title":"Gera\u00e7\u00e3o de Dados Brutos","text":"<p>O primeiro passo no nosso pipeline ETL \u00e9 a gera\u00e7\u00e3o de uma massa de dados simulada que servir\u00e1 como as fontes de dados brutas para o projeto. Este processo \u00e9 executado atrav\u00e9s do script Python <code>scripts/gerador_dados.py</code>, que foi significativamente aprimorado com novas funcionalidades.</p>"},{"location":"pipeline/geracao_dados/#proposito","title":"Prop\u00f3sito","text":"<ul> <li>Simula\u00e7\u00e3o de Fontes: Criar um conjunto de dados realistas que imitam as opera\u00e7\u00f5es de uma cl\u00ednica odontol\u00f3gica, permitindo o desenvolvimento e teste do pipeline ETL sem depender de dados sens\u00edveis ou complexos de sistemas reais.</li> <li>Volumetria e Variedade: Gerar um volume consider\u00e1vel de dados e uma variedade de entidades para demonstrar a capacidade do pipeline em lidar com diferentes tipos de informa\u00e7\u00e3o e escala.</li> <li>Integra\u00e7\u00e3o Completa: Conectar diretamente com banco de dados PostgreSQL e Azure Storage para simular um ambiente de produ\u00e7\u00e3o.</li> </ul>"},{"location":"pipeline/geracao_dados/#novas-funcionalidades-versao-20","title":"Novas Funcionalidades (Vers\u00e3o 2.0)","text":""},{"location":"pipeline/geracao_dados/#integracao-com-banco-de-dados","title":"\ud83d\udd04 Integra\u00e7\u00e3o com Banco de Dados","text":"<ul> <li>Conex\u00e3o PostgreSQL: Conecta automaticamente com o banco usando SQLAlchemy</li> <li>Cria\u00e7\u00e3o de Tabelas: Cria estrutura completa do modelo f\u00edsico</li> <li>Popula\u00e7\u00e3o Autom\u00e1tica: Insere dados diretamente nas tabelas</li> <li>Limpeza Inteligente: Remove dados antigos antes de gerar novos</li> </ul>"},{"location":"pipeline/geracao_dados/#integracao-com-azure-storage","title":"\u2601\ufe0f Integra\u00e7\u00e3o com Azure Storage","text":"<ul> <li>Upload Autom\u00e1tico: Envia CSVs para Azure Blob Storage</li> <li>Configura\u00e7\u00e3o Flex\u00edvel: Usa vari\u00e1veis de ambiente para credenciais</li> <li>Estrutura Organizada: Cria hierarquia de pastas apropriada</li> </ul>"},{"location":"pipeline/geracao_dados/#configuracao-via-ambiente","title":"\ud83d\udd27 Configura\u00e7\u00e3o via Ambiente","text":"<ul> <li>Arquivo .env: Configura\u00e7\u00f5es centralizadas</li> <li>Flexibilidade: Suporte a diferentes ambientes (dev, prod)</li> <li>Seguran\u00e7a: Credenciais n\u00e3o ficam no c\u00f3digo</li> </ul>"},{"location":"pipeline/geracao_dados/#logging-e-monitoramento","title":"\ud83d\udcca Logging e Monitoramento","text":"<ul> <li>Logs Detalhados: Acompanhe cada etapa do processo</li> <li>Tratamento de Erros: Mensagens claras em caso de problemas</li> <li>Progresso Visual: Indicadores de progresso para opera\u00e7\u00f5es longas</li> </ul>"},{"location":"pipeline/geracao_dados/#ferramentas-utilizadas","title":"Ferramentas Utilizadas","text":"<ul> <li>Script Python: <code>scripts/gerador_dados.py</code></li> <li>Bibliotecas:</li> <li><code>Faker</code>: Gera\u00e7\u00e3o de dados fict\u00edcios realistas</li> <li><code>SQLAlchemy</code>: Conex\u00e3o e opera\u00e7\u00f5es com banco de dados</li> <li><code>Azure Storage Blob</code>: Upload para nuvem Azure</li> <li><code>python-dotenv</code>: Gerenciamento de vari\u00e1veis de ambiente</li> <li><code>logging</code>: Sistema de logs robusto</li> </ul>"},{"location":"pipeline/geracao_dados/#entidades-geradas","title":"Entidades Geradas","text":"<p>O script <code>gerador_dados.py</code> cria dados para as seguintes entidades:</p> Entidade Arquivo CSV Descri\u00e7\u00e3o <code>endereco</code> <code>endereco.csv</code> Dados de endere\u00e7o (CEP, rua, cidade, estado) <code>odontologista</code> <code>odontologista.csv</code> Profissionais (nome, CRO, especialidade) <code>paciente</code> <code>paciente.csv</code> Pacientes (nome, CPF, telefone, email) <code>tipo_pagamento</code> <code>tipo_pagamento.csv</code> M\u00e9todos de pagamento (cart\u00e3o, dinheiro, PIX) <code>procedimento</code> <code>procedimento.csv</code> Cat\u00e1logo de procedimentos odontol\u00f3gicos <code>agendamento</code> <code>agendamento.csv</code> Agendamentos de consultas <code>consulta</code> <code>consulta.csv</code> Consultas realizadas <code>consulta_procedimento</code> <code>consulta_procedimento.csv</code> Rela\u00e7\u00e3o consulta \u00d7 procedimento <code>pagamento</code> <code>pagamento.csv</code> Registros de pagamentos <code>log_pagamento</code> <code>log_pagamento.csv</code> Logs de transa\u00e7\u00f5es"},{"location":"pipeline/geracao_dados/#configuracao","title":"Configura\u00e7\u00e3o","text":""},{"location":"pipeline/geracao_dados/#1-variaveis-de-ambiente","title":"1. Vari\u00e1veis de Ambiente","text":"<p>Configure o arquivo <code>.env</code>:</p> <pre><code># Banco de Dados PostgreSQL\nDB_HOST=localhost\nDB_PORT=5432\nDB_NAME=clinica_odonto\nDB_USER=root\nDB_PASSWORD=root\n\n# Azure Storage (opcional)\nAZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=...\nAZURE_CONTAINER_NAME=landingzone\n</code></pre>"},{"location":"pipeline/geracao_dados/#2-parametros-de-volume","title":"2. Par\u00e2metros de Volume","text":"<p>No c\u00f3digo, voc\u00ea pode ajustar a quantidade de dados:</p> <pre><code># Configura\u00e7\u00f5es de volume\nNUM_ENDERECOS = 10000\nNUM_ODONTOLOGISTAS = 50\nNUM_PACIENTES = 20000\nNUM_PROCEDIMENTOS = 200\nNUM_AGENDAMENTOS = 30000\nNUM_CONSULTAS = 25000\nNUM_PAGAMENTOS = 25000\nNUM_LOGS_PAGAMENTO = 30000\n</code></pre>"},{"location":"pipeline/geracao_dados/#processo-de-execucao","title":"Processo de Execu\u00e7\u00e3o","text":""},{"location":"pipeline/geracao_dados/#1-preparacao-do-ambiente","title":"1. Prepara\u00e7\u00e3o do Ambiente","text":"<pre><code># Verificar conex\u00e3o com banco\npython scripts/teste_db.py\n\n# Executar gerador\npython scripts/gerador_dados.py\n</code></pre>"},{"location":"pipeline/geracao_dados/#2-fluxo-de-execucao","title":"2. Fluxo de Execu\u00e7\u00e3o","text":"<ol> <li>Inicializa\u00e7\u00e3o</li> <li>Carrega vari\u00e1veis de ambiente</li> <li>Conecta ao banco PostgreSQL</li> <li> <p>Configura Azure Storage (se dispon\u00edvel)</p> </li> <li> <p>Prepara\u00e7\u00e3o do Banco</p> </li> <li>Cria estrutura de tabelas</li> <li>Limpa dados existentes</li> <li> <p>Configura constraints</p> </li> <li> <p>Gera\u00e7\u00e3o de Dados</p> </li> <li>Gera dados para cada entidade</li> <li>Mant\u00e9m integridade referencial</li> <li> <p>Aplica regras de neg\u00f3cio</p> </li> <li> <p>Persist\u00eancia</p> </li> <li>Insere dados no banco PostgreSQL</li> <li>Exporta para CSV local (<code>data/raw/</code>)</li> <li> <p>Upload para Azure Storage (se configurado)</p> </li> <li> <p>Valida\u00e7\u00e3o</p> </li> <li>Verifica integridade dos dados</li> <li>Gera relat\u00f3rio de contagem</li> <li>Registra logs de sucesso/erro</li> </ol>"},{"location":"pipeline/geracao_dados/#3-saidas-geradas","title":"3. Sa\u00eddas Geradas","text":""},{"location":"pipeline/geracao_dados/#arquivos-locais","title":"Arquivos Locais","text":"<pre><code>data/raw/\n\u251c\u2500\u2500 agendamento.csv\n\u251c\u2500\u2500 consulta.csv\n\u251c\u2500\u2500 consulta_procedimento.csv\n\u251c\u2500\u2500 endereco.csv\n\u251c\u2500\u2500 log_pagamento.csv\n\u251c\u2500\u2500 odontologista.csv\n\u251c\u2500\u2500 paciente.csv\n\u251c\u2500\u2500 pagamento.csv\n\u251c\u2500\u2500 procedimento.csv\n\u2514\u2500\u2500 tipo_pagamento.csv\n</code></pre>"},{"location":"pipeline/geracao_dados/#banco-postgresql","title":"Banco PostgreSQL","text":"<ul> <li>Tabelas populadas com dados</li> <li>\u00cdndices e constraints aplicados</li> <li>Dados prontos para an\u00e1lise</li> </ul>"},{"location":"pipeline/geracao_dados/#azure-storage-se-configurado","title":"Azure Storage (se configurado)","text":"<pre><code>landingzone/\n\u2514\u2500\u2500 raw/\n    \u251c\u2500\u2500 agendamento.csv\n    \u251c\u2500\u2500 consulta.csv\n    \u2514\u2500\u2500 ... (outros arquivos)\n</code></pre>"},{"location":"pipeline/geracao_dados/#logs-e-monitoramento","title":"Logs e Monitoramento","text":""},{"location":"pipeline/geracao_dados/#exemplo-de-log-de-execucao","title":"Exemplo de Log de Execu\u00e7\u00e3o","text":"<pre><code>2024-12-25 10:30:15 - INFO - Iniciando gera\u00e7\u00e3o de dados...\n2024-12-25 10:30:16 - INFO - Conectado ao banco PostgreSQL\n2024-12-25 10:30:17 - INFO - Azure Storage configurado\n2024-12-25 10:30:18 - INFO - Criando e limpando tabelas...\n2024-12-25 10:30:25 - INFO - Gerando 10000 endere\u00e7os...\n2024-12-25 10:30:28 - INFO - Gerando 50 odontologistas...\n2024-12-25 10:30:30 - INFO - \u2705 Dados inseridos com sucesso no banco\n2024-12-25 10:30:32 - INFO - \u2705 CSVs exportados para data/raw/\n2024-12-25 10:30:35 - INFO - \u2705 Upload para Azure Storage conclu\u00eddo\n2024-12-25 10:30:35 - INFO - \ud83c\udf89 Processo conclu\u00eddo com sucesso!\n</code></pre>"},{"location":"pipeline/geracao_dados/#tratamento-de-erros","title":"Tratamento de Erros","text":""},{"location":"pipeline/geracao_dados/#problemas-comuns","title":"Problemas Comuns","text":"<ol> <li> <p>Erro de Conex\u00e3o com Banco <pre><code>ERROR - Erro ao conectar ao banco: connection refused\n# Solu\u00e7\u00e3o: Verificar se PostgreSQL est\u00e1 rodando\ndocker-compose -f docker/docker-compose.yml up -d\n</code></pre></p> </li> <li> <p>Erro do Azure Storage <pre><code>WARNING - Azure Storage n\u00e3o configurado, pulando upload\n# Solu\u00e7\u00e3o: Configurar AZURE_STORAGE_CONNECTION_STRING no .env\n</code></pre></p> </li> <li> <p>Erro de Depend\u00eancias <pre><code>ModuleNotFoundError: No module named 'azure'\n# Solu\u00e7\u00e3o: Instalar depend\u00eancias\npip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"pipeline/geracao_dados/#para-executar","title":"Para Executar","text":"<pre><code># Execu\u00e7\u00e3o simples\npython scripts/gerador_dados.py\n\n# Com logs detalhados\npython scripts/gerador_dados.py 2&gt;&amp;1 | tee gerador.log\n\n# Verificar resultados\nls -la data/raw/\npsql -h localhost -U root -d clinica_odonto -c \"SELECT COUNT(*) FROM paciente;\"\n</code></pre> <p>Este processo garante que tenhamos uma base s\u00f3lida de dados para alimentar todo o pipeline ETL, desde a camada Landing at\u00e9 o Data Warehouse final.</p>"},{"location":"pipeline/notebook_bronze_silver/","title":"Stage 2: Transforma\u00e7\u00e3o de Bronze para Silver","text":"<p>O <code>notebooks/notebook_bronze_silver.ipynb</code> \u00e9 o cora\u00e7\u00e3o do nosso pipeline de ETL, onde os dados da Camada Bronze s\u00e3o submetidos a um processo rigoroso de limpeza, valida\u00e7\u00e3o e transforma\u00e7\u00e3o. O objetivo final \u00e9 produzir dados de alta qualidade na Camada Silver (<code>data/silver/</code>), prontos para an\u00e1lises e modelagem dimensional.</p> <p>Objetivo:</p> <ul> <li>Limpeza de Dados: Tratar valores nulos, inconsist\u00eancias, duplicatas e erros de digita\u00e7\u00e3o.</li> <li>Padroniza\u00e7\u00e3o de Formatos: Uniformizar o formato de datas, strings e outros tipos de dados.</li> <li>Enriquecimento de Dados: Adicionar informa\u00e7\u00f5es derivadas ou combinar datasets para criar um conjunto de dados mais completo e \u00fatil.</li> <li>Aplica\u00e7\u00e3o de Regras de Neg\u00f3cio: Implementar l\u00f3gicas espec\u00edficas que garantam a consist\u00eancia e a validade dos dados conforme os requisitos do neg\u00f3cio da cl\u00ednica.</li> </ul> <p>Ferramentas Utilizadas:</p> <ul> <li>Jupyter Notebook: <code>notebooks/notebook_bronze_silver.ipynb</code></li> <li>Python: Linguagem de programa\u00e7\u00e3o principal.</li> <li>Pandas: Biblioteca essencial para manipula\u00e7\u00e3o e transforma\u00e7\u00e3o de DataFrames.</li> </ul> <p>Entrada:</p> <ul> <li>Arquivos CSV localizados na pasta <code>data/bronze/</code> (dados ap\u00f3s a ingest\u00e3o b\u00e1sica).</li> </ul> <p>Sa\u00edda:</p> <ul> <li>Arquivos CSV transformados e limpos, salvos na pasta <code>data/silver/</code>.</li> </ul> <p>Processo Detalhado e Principais Transforma\u00e7\u00f5es:</p> <p>Para cada entidade (Pacientes, Consultas, Odontologistas, etc.), o notebook <code>notebook_bronze_silver.ipynb</code> executa uma s\u00e9rie de transforma\u00e7\u00f5es. Abaixo est\u00e3o os tipos de opera\u00e7\u00f5es que tipicamente s\u00e3o realizadas:</p> <ol> <li> <p>Carregamento dos Dados:</p> <ul> <li>Todos os arquivos CSV da pasta <code>data/bronze/</code> s\u00e3o carregados em DataFrames do Pandas.</li> <li>Exemplo: Carregamento de <code>paciente.csv</code>, <code>consulta.csv</code>, <code>agendamento.csv</code>, <code>procedimento.csv</code>, <code>pagamento.csv</code>, <code>tipo_pagamento.csv</code>, <code>odontologista.csv</code>, <code>endereco.csv</code>, <code>consulta_procedimento.csv</code>, <code>log_pagamento.csv</code>.</li> </ul> </li> <li> <p>Tratamento de Valores Nulos (Missing Values):</p> <ul> <li>Identifica\u00e7\u00e3o: Verifica\u00e7\u00e3o de colunas com alta porcentagem de valores nulos.</li> <li>Imputa\u00e7\u00e3o/Remo\u00e7\u00e3o: Decis\u00f5es s\u00e3o tomadas com base na relev\u00e2ncia da coluna:<ul> <li>Preencher nulos com valores padr\u00e3o (e.g., <code>telefone</code> com \"N\u00e3o Informado\", <code>email</code> com \"email_nao_informado@dominio.com\").</li> <li>Preencher valores num\u00e9ricos nulos com m\u00e9dia, mediana ou zero, se aplic\u00e1vel.</li> <li>Remover linhas inteiras se valores cr\u00edticos (como <code>id_paciente</code> ou <code>data_consulta</code>) estiverem nulos.</li> </ul> </li> <li>Exemplo: <code>df_pacientes['telefone'].fillna('N\u00e3o Informado', inplace=True)</code>.</li> </ul> </li> <li> <p>Padroniza\u00e7\u00e3o de Formatos de Dados:</p> <ul> <li>Datas: Convers\u00e3o de colunas de data (e.g., <code>data_consulta</code>, <code>data_agendamento</code>, <code>data_nascimento</code>) para o formato <code>YYYY-MM-DD</code> e tipo <code>datetime</code>.<ul> <li>Exemplo: <code>pd.to_datetime(df_consultas['data_consulta'], errors='coerce')</code>.</li> </ul> </li> <li>Strings: Padroniza\u00e7\u00e3o de case (min\u00fasculas, mai\u00fasculas, t\u00edtulo), remo\u00e7\u00e3o de espa\u00e7os extras, caracteres especiais indesejados.<ul> <li>Exemplo: <code>df_pacientes['nome'].str.strip().str.title()</code>.</li> </ul> </li> <li>Valores Num\u00e9ricos: Garantir que colunas como <code>valor_pago</code>, <code>custo</code> estejam no tipo num\u00e9rico correto (float ou int) e sem caracteres inv\u00e1lidos.</li> </ul> </li> <li> <p>Remo\u00e7\u00e3o de Duplicatas:</p> <ul> <li>Identifica\u00e7\u00e3o e remo\u00e7\u00e3o de registros duplicados em tabelas que devem ter entradas \u00fanicas (e.g., <code>paciente</code>, <code>odontologista</code>, <code>procedimento</code>).</li> <li>Exemplo: <code>df_pacientes.drop_duplicates(subset=['id_paciente'], inplace=True)</code>.</li> </ul> </li> <li> <p>Valida\u00e7\u00e3o e Consist\u00eancia de Dados:</p> <ul> <li>Verifica\u00e7\u00f5es de sanidade:<ul> <li>Idades de pacientes razo\u00e1veis (e.g., n\u00e3o negativas ou muito altas).</li> <li>Valores de pagamento positivos.</li> <li>Integridade referencial b\u00e1sica (garantir que IDs referenciados em uma tabela existam em outra, antes de uni-las).</li> </ul> </li> <li>Corre\u00e7\u00e3o de inconsist\u00eancias l\u00f3gicas se identificadas.</li> </ul> </li> <li> <p>Enriquecimento de Dados e Cria\u00e7\u00e3o de Novas Features:</p> <ul> <li>C\u00e1lculo de Idade: Adicionar uma coluna <code>idade</code> na tabela de pacientes.</li> <li>Deriva\u00e7\u00e3o de Tempo: Extrair <code>ano</code>, <code>mes</code>, <code>dia_semana</code> de colunas de data.</li> <li>Combinar Dados: Realizar <code>merges</code> ou <code>joins</code> entre DataFrames para consolidar informa\u00e7\u00f5es.<ul> <li>Exemplo: Unir <code>df_pagamentos</code> com <code>df_tipo_pagamento</code> para adicionar a descri\u00e7\u00e3o do tipo de pagamento diretamente nos registros de pagamento.</li> <li>Exemplo: Unir <code>df_consultas</code> com <code>df_pacientes</code> e <code>df_odontologistas</code> para consolidar informa\u00e7\u00f5es em uma \u00fanica tabela de eventos de consulta.</li> </ul> </li> </ul> </li> <li> <p>Persist\u00eancia na Camada Silver:</p> <ul> <li>Ap\u00f3s todas as transforma\u00e7\u00f5es, os DataFrames resultantes s\u00e3o salvos como novos arquivos CSV na pasta <code>data/silver/</code>. Cada arquivo reflete a entidade correspondente, agora limpa e padronizada.</li> <li>Exemplo: <code>df_pacientes_clean.to_csv('data/silver/paciente_silver.csv', index=False)</code>.</li> </ul> </li> </ol> <p>Esta etapa \u00e9 a mais intensiva em termos de processamento de dados e garante que a camada Gold receber\u00e1 dados de alta qualidade e consist\u00eancia para a modelagem anal\u00edtica.</p>"},{"location":"pipeline/notebook_landing_bronze/","title":"Stage 1: Ingest\u00e3o de Landing para Bronze","text":"<p>O primeiro est\u00e1gio de processamento no nosso pipeline ETL \u00e9 a movimenta\u00e7\u00e3o dos dados brutos da Camada Landing (<code>data/raw/</code>) para a Camada Bronze (<code>data/bronze/</code>). Este processo \u00e9 orquestrado e executado atrav\u00e9s do notebook Jupyter <code>notebooks/notebook_landing_bronze.ipynb</code>.</p> <p>Objetivo:</p> <ul> <li>Ingest\u00e3o Segura: Garantir que todos os arquivos CSV brutos da camada Landing sejam lidos com sucesso.</li> <li>Valida\u00e7\u00e3o de Leitura: Confirmar que os dados podem ser carregados corretamente em DataFrames do Pandas, identificando problemas b\u00e1sicos de formato de arquivo, se existirem.</li> <li>Persist\u00eancia Consistente: Salvar os dados lidos em novos arquivos CSV na camada Bronze, estabelecendo um ponto de partida limpo e validado para as transforma\u00e7\u00f5es futuras.</li> </ul> <p>Ferramentas Utilizadas:</p> <ul> <li>Jupyter Notebook: <code>notebooks/notebook_landing_bronze.ipynb</code></li> <li>Python: Linguagem de programa\u00e7\u00e3o para o script.</li> <li>Pandas: Biblioteca para manipula\u00e7\u00e3o e carregamento de DataFrames.</li> </ul> <p>Entrada:</p> <ul> <li>Arquivos CSV localizados na pasta <code>data/raw/</code> (provenientes da gera\u00e7\u00e3o de dados brutos).</li> </ul> <p>Sa\u00edda:</p> <ul> <li>Arquivos CSV id\u00eanticos aos de entrada, por\u00e9m agora salvos na pasta <code>data/bronze/</code>.</li> </ul> <p>Processo Detalhado e Transforma\u00e7\u00f5es Aplicadas:</p> <p>O <code>notebook_landing_bronze.ipynb</code> realiza as seguintes opera\u00e7\u00f5es para cada arquivo CSV presente na camada Landing:</p> <ol> <li>Itera\u00e7\u00e3o sobre Arquivos Raw: O notebook itera sobre todos os arquivos <code>.csv</code> encontrados dentro da pasta <code>data/raw/</code>.</li> <li>Leitura do CSV: Para cada arquivo, o Pandas \u00e9 utilizado para ler o conte\u00fado do CSV em um DataFrame.     <pre><code>import pandas as pd\nimport os\n\n# Exemplo simplificado de leitura\nfile_path_raw = 'data/raw/paciente.csv' # Caminho real seria din\u00e2mico\ndf = pd.read_csv(file_path_raw)\n</code></pre></li> <li>Cria\u00e7\u00e3o da Pasta Bronze (se necess\u00e1rio): Verifica se a pasta <code>data/bronze/</code> existe. Se n\u00e3o, ela \u00e9 criada para armazenar os arquivos processados.</li> <li>Persist\u00eancia na Camada Bronze: O DataFrame lido \u00e9 ent\u00e3o salvo como um novo arquivo CSV no caminho correspondente dentro da pasta <code>data/bronze/</code>. Isso assegura que a camada Bronze contenha uma c\u00f3pia \"limpa\" dos dados brutos, pronta para a pr\u00f3xima etapa.     <pre><code>output_path_bronze = 'data/bronze/paciente.csv' # Caminho real seria din\u00e2mico\ndf.to_csv(output_path_bronze, index=False)\n</code></pre></li> <li>Verifica\u00e7\u00e3o B\u00e1sica (Impl\u00edcita): Embora n\u00e3o haja transforma\u00e7\u00f5es complexas ou valida\u00e7\u00f5es de dados nesta etapa, a simples opera\u00e7\u00e3o de leitura e escrita do Pandas serve como uma valida\u00e7\u00e3o impl\u00edcita de que os arquivos n\u00e3o est\u00e3o corrompidos e s\u00e3o leg\u00edveis. Qualquer erro na leitura ou escrita seria sinalizado neste ponto.</li> </ol> <p>Esta etapa \u00e9 crucial para estabelecer a camada Bronze como um ponto de controle e garantir a integridade b\u00e1sica dos dados antes de qualquer transforma\u00e7\u00e3o complexa ser aplicada.</p>"},{"location":"pipeline/notebook_silver_gold/","title":"Stage 3: Modelagem e Carregamento de Silver para Gold","text":"<p>O <code>notebooks/notebook_silver_gold.ipynb</code> \u00e9 o est\u00e1gio final do nosso pipeline ETL, respons\u00e1vel por transformar os dados limpos e padronizados da Camada Silver em um modelo dimensional otimizado para an\u00e1lise (Camada Gold). Esta etapa envolve a cria\u00e7\u00e3o de tabelas de dimens\u00e3o e tabelas de fato, e o carregamento desses dados em um banco de dados relacional (PostgreSQL, conforme o projeto).</p> <p>Objetivo:</p> <ul> <li>Modelagem Dimensional: Construir um esquema estrela (Star Schema) ou floco de neve (Snowflake Schema) que seja intuitivo para usu\u00e1rios de neg\u00f3cio e eficiente para consultas anal\u00edticas.</li> <li>Agrega\u00e7\u00e3o de Dados: Realizar agrega\u00e7\u00f5es e sumariza\u00e7\u00f5es necess\u00e1rias para as m\u00e9tricas de neg\u00f3cio.</li> <li>Carregamento no Data Warehouse: Persistir os dados modelados em um banco de dados relacional que servir\u00e1 como o Data Warehouse final para consumo por ferramentas de BI.</li> </ul> <p>Ferramentas Utilizadas:</p> <ul> <li>Jupyter Notebook: <code>notebooks/notebook_silver_gold.ipynb</code></li> <li>Python: Linguagem de programa\u00e7\u00e3o principal.</li> <li>Pandas: Para manipula\u00e7\u00e3o e transforma\u00e7\u00e3o de DataFrames.</li> <li>SQLAlchemy / Psycopg2: Para conex\u00e3o e intera\u00e7\u00e3o com o banco de dados PostgreSQL.</li> <li>SQL: Scripts <code>scripts/modelo_fisico.sql</code> e <code>scripts/modelo_dimensional.sql</code> para cria\u00e7\u00e3o da estrutura do banco de dados.</li> </ul> <p>Entrada:</p> <ul> <li>Arquivos CSV limpos e transformados, localizados na pasta <code>data/silver/</code>.</li> </ul> <p>Sa\u00edda:</p> <ul> <li>Tabelas de fato e dimens\u00e3o populadas no banco de dados PostgreSQL (camada Gold).</li> <li>(Opcional: arquivos Parquet ou CSV da camada Gold em <code>data/gold/</code> para persist\u00eancia local antes do carregamento no DB).</li> </ul> <p>Processo Detalhado e Principais Transforma\u00e7\u00f5es:</p> <p>O notebook <code>notebook_silver_gold.ipynb</code> executa as seguintes opera\u00e7\u00f5es:</p> <ol> <li> <p>Carregamento dos Dados da Camada Silver:</p> <ul> <li>Todos os arquivos CSV da pasta <code>data/silver/</code> s\u00e3o carregados em DataFrames do Pandas. Isso inclui dados j\u00e1 limpos de pacientes, consultas, pagamentos, procedimentos, etc.</li> </ul> </li> <li> <p>Prepara\u00e7\u00e3o das Tabelas de Dimens\u00e3o:</p> <ul> <li>Para cada dimens\u00e3o (por exemplo, <code>dim_paciente</code>, <code>dim_odontologista</code>, <code>dim_procedimento</code>, <code>dim_tempo</code>, <code>dim_tipo_pagamento</code>), os dados relevantes s\u00e3o selecionados e transformados para se tornarem atributos da dimens\u00e3o.</li> <li>Remo\u00e7\u00e3o de Duplicatas: Garante que cada dimens\u00e3o contenha apenas entradas \u00fanicas.</li> <li>Cria\u00e7\u00e3o de Chaves Suplicadas (Surrogate Keys - Opcional, mas comum): Se n\u00e3o houver chaves de neg\u00f3cio adequadas, novas chaves num\u00e9ricas sequenciais podem ser geradas para as dimens\u00f5es.</li> <li>Modelagem da Dimens\u00e3o Tempo: Uma dimens\u00e3o de tempo pode ser constru\u00edda a partir das datas das transa\u00e7\u00f5es (consultas, agendamentos), extraindo atributos como ano, m\u00eas, dia, dia da semana, trimestre, etc.</li> </ul> </li> <li> <p>Constru\u00e7\u00e3o da Tabela de Fato:</p> <ul> <li>A tabela de fato \u00e9 constru\u00edda a partir dos dados transacionais (principalmente de consultas e pagamentos) da camada Silver.</li> <li>Jun\u00e7\u00e3o com Dimens\u00f5es: As chaves de neg\u00f3cio da tabela de fato s\u00e3o usadas para buscar as chaves suplicadas das dimens\u00f5es correspondentes. Isso conecta a tabela de fato \u00e0s suas dimens\u00f5es.</li> <li>C\u00e1lculo de M\u00e9tricas: As m\u00e9tricas de neg\u00f3cio s\u00e3o calculadas e agregadas. Exemplos:<ul> <li><code>valor_total_consulta</code> (soma dos pagamentos e procedimentos de uma consulta)</li> <li><code>quantidade_procedimentos</code></li> <li><code>duracao_consulta</code> (se aplic\u00e1vel)</li> </ul> </li> <li>A tabela de fato \u00e9 projetada para ser granular, contendo uma linha por evento de neg\u00f3cio (ex: uma consulta, um pagamento).</li> </ul> </li> <li> <p>Cria\u00e7\u00e3o/Atualiza\u00e7\u00e3o da Estrutura do Banco de Dados:</p> <ul> <li>O notebook pode primeiro verificar se as tabelas da camada Gold existem no banco de dados.</li> <li>\u00c9 comum que os scripts SQL <code>scripts/modelo_fisico.sql</code> (para criar o DB, schemas, etc.) e <code>scripts/modelo_dimensional.sql</code> (para criar as tabelas de fato e dimens\u00e3o) sejam executados antes de carregar os dados. O notebook pode orquestrar essa execu\u00e7\u00e3o via <code>psycopg2</code> ou <code>SQLAlchemy</code>.</li> </ul> </li> <li> <p>Carregamento dos Dados no Banco de Dados (Load):</p> <ul> <li>As tabelas de dimens\u00e3o s\u00e3o carregadas primeiro no PostgreSQL.</li> <li>A tabela de fato \u00e9 carregada em seguida, ap\u00f3s suas dimens\u00f5es correspondentes j\u00e1 estarem populadas.</li> <li>Isso pode ser feito usando <code>df.to_sql()</code> do Pandas (via SQLAlchemy) ou inser\u00e7\u00f5es diretas com <code>psycopg2</code>.</li> <li>Exemplo:     <pre><code>from sqlalchemy import create_engine\nengine = create_engine('postgresql://user:password@host:port/database') # Ajustar credenciais\n\n# Carregar dimens\u00e3o paciente\ndf_dim_paciente.to_sql('dim_paciente', engine, if_exists='replace', index=False)\n\n# Carregar fato consultas_e_pagamentos\ndf_fato_consultas.to_sql('fato_consultas_e_pagamentos', engine, if_exists='replace', index=False)\n</code></pre></li> <li>\u00c9 importante considerar a estrat\u00e9gia de carregamento (completa ou incremental, para projetos mais complexos). Neste caso, um carregamento completo (<code>if_exists='replace'</code>) para fins de demonstra\u00e7\u00e3o \u00e9 comum.</li> </ul> </li> </ol> <p>Este est\u00e1gio finaliza o processo de ETL, disponibilizando os dados em um formato que \u00e9 diretamente consum\u00edvel para relat\u00f3rios e dashboards anal\u00edticos, fechando o ciclo da jornada do dado.</p>"}]}